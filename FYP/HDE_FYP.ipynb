{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sevinpr/Synthetic-Sniffer/blob/main/FYP/HDE_FYP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading dataset from kaggle"
      ],
      "metadata": {
        "id": "UfVFJ623aQ_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StyleGAN & Flickr for training and validation"
      ],
      "metadata": {
        "id": "WGTiKxuEaxiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Specify the desired download path\n",
        "download_path = \"/content/dataset\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Download the dataset to the specified path (initially downloaded to Kaggle cache)\n",
        "path = kagglehub.dataset_download(\"xhlulu/140k-real-and-fake-faces\")\n",
        "print(\"Path to dataset files (cache):\", path)  # 'path' points to the cache\n",
        "\n",
        "# Define source (Kaggle cache) and destination paths\n",
        "source_path = '/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake'  # Adjust if version changes\n",
        "destination_path = '/content/dataset'\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(destination_path, exist_ok=True)\n",
        "\n",
        "# Iterate through all files and directories in the source path (cache)\n",
        "for item in os.listdir(source_path):\n",
        "    source_item_path = os.path.join(source_path, item)\n",
        "    destination_item_path = os.path.join(destination_path, item)\n",
        "\n",
        "    # If the item is a directory, use copytree; otherwise, use copy2\n",
        "    if os.path.isdir(source_item_path):\n",
        "        shutil.copytree(source_item_path, destination_item_path, dirs_exist_ok=True)\n",
        "    else:\n",
        "        shutil.copy2(source_item_path, destination_item_path)\n",
        "\n",
        "print(f\"Files copied from '{source_path}' to '{destination_path}' successfully.\")\n"
      ],
      "metadata": {
        "id": "Y8GL2Eb6IEaE",
        "outputId": "4e5f1703-1996-46a1-94e0-56d21f232bc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files (cache): /kaggle/input/140k-real-and-fake-faces\n",
            "Files copied from '/kaggle/input/140k-real-and-fake-faces/real_vs_fake/real-vs-fake' to '/content/dataset' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRO-GAN dataset for testing"
      ],
      "metadata": {
        "id": "TKyNwAsLatKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mayankjha146025/fake-face-images-generated-from-different-gans\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Check if the dataset contains an 'images' folder\n",
        "# If not, adjust the folder name accordingly\n",
        "if not os.path.exists(os.path.join(path, 'images')):\n",
        "    # List all folders in the dataset path\n",
        "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "    if folders:\n",
        "        # Assume the first folder in the list is the correct one\n",
        "        image_folder_path = os.path.join(path, folders[0])\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No subfolders found in the dataset directory.\")\n",
        "else:\n",
        "    image_folder_path = os.path.join(path, 'images')\n",
        "\n",
        "# Create an ImageFolder dataset object to easily access the data\n",
        "dataset = datasets.ImageFolder(image_folder_path)\n",
        "\n",
        "# Print the total number of samples in the dataset\n",
        "print(\"Total number of data samples:\", len(dataset))"
      ],
      "metadata": {
        "id": "0LfBvm8-1zb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swap the testing folder to test with PROGAN"
      ],
      "metadata": {
        "id": "NYTscHL-a7N2"
      }
    },
    {
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# 1. Delete content in /content/dataset/real_vs_fake/real-vs-fake/test/fake\n",
        "fake_folder = '/content/dataset/real_vs_fake/real-vs-fake/test/fake'\n",
        "for filename in os.listdir(fake_folder):\n",
        "    file_path = os.path.join(fake_folder, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "# 2. Randomly delete 2100 images from /content/dataset/real_vs_fake/real-vs-fake/test/real\n",
        "real_folder = '/content/dataset/real_vs_fake/real-vs-fake/test/real'\n",
        "real_images = os.listdir(real_folder)\n",
        "images_to_delete = random.sample(real_images, 2900)\n",
        "for image in images_to_delete:\n",
        "    image_path = os.path.join(real_folder, image)\n",
        "    os.remove(image_path)\n",
        "\n",
        "# 3. Move images from /content/fake-face-images-generated-from-different-gans/GAN_Generated_Fake_Images/ProGAN_128x128 to /content/dataset/real_vs_fake/real-vs-fake/test/fake\n",
        "source_folder = '/content/fake-face-images-generated-from-different-gans/GAN_Generated_Fake_Images/ProGAN_128x128'\n",
        "for filename in os.listdir(source_folder):\n",
        "    source_path = os.path.join(source_folder, filename)\n",
        "    destination_path = os.path.join(fake_folder, filename)\n",
        "    shutil.move(source_path, destination_path)\n",
        "\n",
        "print(\"Operations completed successfully.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "eRLw5sw948lC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Augmentation"
      ],
      "metadata": {
        "id": "KnPMRa4gdWxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er3cW9Drc-YI",
        "outputId": "84d140a8-db4a-4872-820d-4eee796aa372"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_quantity(dataset, dataset_name):\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"  Total samples: {len(dataset)}\")\n",
        "    # If the dataset is an instance of ImageFolder, it contains a 'targets' attribute.\n",
        "    if hasattr(dataset, 'targets'):\n",
        "        from collections import Counter\n",
        "        counts = Counter(dataset.targets)\n",
        "        # Map numeric labels back to class names using class_to_idx\n",
        "        idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
        "        print(\"  Class distribution:\")\n",
        "        for cls, count in counts.items():\n",
        "            print(f\"    {idx_to_class[cls]}: {count}\")\n",
        "    else:\n",
        "        print(\"  No target information available.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R_6TcKhcHOWG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define data transforms (including augmentations)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Adjust these paths to your dataset locations in Colab\n",
        "train_data = datasets.ImageFolder('/content/dataset/train', transform=train_transforms)\n",
        "val_data   = datasets.ImageFolder('/content/dataset/valid', transform=val_test_transforms)\n",
        "test_data  = datasets.ImageFolder('/content/dataset/test', transform=val_test_transforms)\n",
        "\n",
        "# Check data quantity for each dataset\n",
        "check_data_quantity(train_data, \"Train\")\n",
        "check_data_quantity(val_data, \"Validation\")\n",
        "check_data_quantity(test_data, \"Test\")\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4)\n",
        "test_loader  = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ],
      "metadata": {
        "id": "Oq4Zhw3CdFdh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5622d7de-c69b-4d5a-fcea-79400d93943a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: Train\n",
            "  Total samples: 100000\n",
            "  Class distribution:\n",
            "    fake: 50000\n",
            "    real: 50000\n",
            "Dataset: Validation\n",
            "  Total samples: 20000\n",
            "  Class distribution:\n",
            "    fake: 10000\n",
            "    real: 10000\n",
            "Dataset: Test\n",
            "  Total samples: 20000\n",
            "  Class distribution:\n",
            "    fake: 10000\n",
            "    real: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the target size (consistent with your model's FC layer)\n",
        "IMAGE_SIZE = 256\n",
        "\n",
        "# Define data transforms (including augmentations and RESIZING)\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
        "])\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "train_data_path = 'preprocessed_train_data.pt'\n",
        "val_data_path = 'preprocessed_val_data.pt'\n",
        "test_data_path = 'preprocessed_test_data.pt'\n",
        "\n",
        "if os.path.exists(train_data_path) and os.path.exists(val_data_path) and os.path.exists(test_data_path):\n",
        "    # Load preprocessed data\n",
        "    train_data = torch.load(train_data_path)\n",
        "    val_data = torch.load(val_data_path)\n",
        "    test_data = torch.load(test_data_path)\n",
        "else:\n",
        "    # Create and save preprocessed data\n",
        "    train_data = datasets.ImageFolder('/content/dataset/train', transform=train_transforms)\n",
        "    val_data = datasets.ImageFolder('/content/dataset/valid', transform=val_test_transforms)\n",
        "    test_data = datasets.ImageFolder('/content/dataset/test', transform=val_test_transforms)\n",
        "\n",
        "    torch.save(train_data, train_data_path)\n",
        "    torch.save(val_data, val_data_path)\n",
        "    torch.save(test_data, test_data_path)\n",
        "\n",
        "# --- Re-define the DataLoaders ---\n",
        "batch_size = 128\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n"
      ],
      "metadata": {
        "id": "Xkwlv1Ern-7J"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the training dataset to check the shape after transforms\n",
        "print(\"Checking shape of a sample image after preprocessing:\")\n",
        "# Retrieve the first sample (image tensor and its label) from the train_data\n",
        "sample_image_tensor, sample_label = train_data[0]\n",
        "\n",
        "# The transforms.ToTensor() step moves the channel dimension to the front (C, H, W)\n",
        "# The transforms.Normalize() step does not change the shape.\n",
        "print(f\"  Type of the preprocessed image: {type(sample_image_tensor)}\")\n",
        "print(f\"  Shape of the preprocessed image (Channels, Height, Width): {sample_image_tensor.shape}\")\n",
        "print(f\"  Label of the sample: {sample_label} (Class: {train_data.classes[sample_label]})\") # Optional: see label\n",
        "\n",
        "# You can also check the shape of a batch from the DataLoader\n",
        "print(\"\\nChecking shape of a batch from the DataLoader:\")\n",
        "# Get one batch of data from the train_loader\n",
        "images_batch, labels_batch = next(iter(train_loader))\n",
        "print(f\"  Shape of an image batch (Batch Size, Channels, Height, Width): {images_batch.shape}\")\n",
        "print(f\"  Shape of a label batch (Batch Size): {labels_batch.shape}\") # Optional: see labels shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6CGBBMlhgv",
        "outputId": "61c43fd6-4da5-4396-8d6b-2e983e11fcda"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking shape of a sample image after preprocessing:\n",
            "  Type of the preprocessed image: <class 'torch.Tensor'>\n",
            "  Shape of the preprocessed image (Channels, Height, Width): torch.Size([3, 256, 256])\n",
            "  Label of the sample: 0 (Class: fake)\n",
            "\n",
            "Checking shape of a batch from the DataLoader:\n",
            "  Shape of an image batch (Batch Size, Channels, Height, Width): torch.Size([128, 3, 256, 256])\n",
            "  Shape of a label batch (Batch Size): torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the test dataset to check the shape after transforms\n",
        "print(\"\\nChecking shape of a sample TEST image after preprocessing:\")\n",
        "# Retrieve the first sample (image tensor and its label) from the test_data\n",
        "sample_test_image_tensor, sample_test_label = test_data[0]\n",
        "\n",
        "# The transforms.ToTensor() step moves the channel dimension to the front (C, H, W)\n",
        "# The transforms.Normalize() step does not change the shape.\n",
        "print(f\"  Type of the preprocessed test image: {type(sample_test_image_tensor)}\")\n",
        "print(f\"  Shape of the preprocessed test image (Channels, Height, Width): {sample_test_image_tensor.shape}\")\n",
        "print(f\"  Label of the test sample: {sample_test_label} (Class: {test_data.classes[sample_test_label]})\") # Optional: see label\n",
        "\n",
        "# You can also check the shape of a batch from the test DataLoader\n",
        "print(\"\\nChecking shape of a batch from the TEST DataLoader:\")\n",
        "# Get one batch of data from the test_loader\n",
        "test_images_batch, test_labels_batch = next(iter(test_loader))\n",
        "print(f\"  Shape of a test image batch (Batch Size, Channels, Height, Width): {test_images_batch.shape}\")\n",
        "print(f\"  Shape of a test label batch (Batch Size): {test_labels_batch.shape}\") # Optional: see labels shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyXkbx1qnppv",
        "outputId": "8e456a2c-4da7-4d42-992b-de74ca502c92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking shape of a sample TEST image after preprocessing:\n",
            "  Type of the preprocessed test image: <class 'torch.Tensor'>\n",
            "  Shape of the preprocessed test image (Channels, Height, Width): torch.Size([3, 256, 256])\n",
            "  Label of the test sample: 0 (Class: fake)\n",
            "\n",
            "Checking shape of a batch from the TEST DataLoader:\n",
            "  Shape of a test image batch (Batch Size, Channels, Height, Width): torch.Size([128, 3, 256, 256])\n",
            "  Shape of a test label batch (Batch Size): torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First-Level Disentanglement: DR-GAN Module"
      ],
      "metadata": {
        "id": "32q4doxSdf0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "class DRGANEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGANEncoder, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Simple CNN architecture;\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # 256->128\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128->64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 64->32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # 32->16\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Linear(512*16*16, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        latent = self.fc(x)\n",
        "        return latent\n",
        "\n",
        "class DRGANEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128, freeze_backbone=True):\n",
        "        super(DRGANEncoder, self).__init__()\n",
        "        # Example using ResNet50\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "        # Remove the original classifier\n",
        "        self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for param in self.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Adjust the input dimension of the FC layer based on backbone output\n",
        "        # ResNet50 output feature dim is 2048\n",
        "        num_ftrs = backbone.fc.in_features # or simply 2048 for ResNet50\n",
        "        self.fc = nn.Linear(num_ftrs, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1) # Flatten features\n",
        "        latent = self.fc(x)\n",
        "        return latent\n",
        "\n",
        "class DRGANDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGANDecoder, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 512*16*16)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 16->32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 32->64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64->128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # 128->256\n",
        "            nn.Tanh()  # Assuming image pixels in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = self.fc(latent)\n",
        "        x = x.view(x.size(0), 512, 16, 16)\n",
        "        rec = self.deconv(x)\n",
        "        return rec\n",
        "\n",
        "class DRGAN(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGAN, self).__init__()\n",
        "        self.encoder = DRGANEncoder(latent_dim=latent_dim)\n",
        "        self.decoder = DRGANDecoder(latent_dim=latent_dim)\n",
        "        # A simple classifier to differentiate real vs. fake based on latent code\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 2)  # 2 classes: real, fake\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        rec = self.decoder(latent)\n",
        "        class_logits = self.classifier(latent)\n",
        "        return latent, rec, class_logits\n"
      ],
      "metadata": {
        "id": "djTfRmr-dhRX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Second-Level Analysis: Multi-Pathway Modules"
      ],
      "metadata": {
        "id": "RYcu4RKEdlb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SemanticPathway(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(SemanticPathway, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        return self.net(latent)\n",
        "\n",
        "class FrequencyPathway(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(FrequencyPathway, self).__init__()\n",
        "        # Here we simulate frequency analysis by applying an FFT (could be extended)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        # Compute a pseudo frequency representation\n",
        "        # Note: For a real implementation, consider applying a Fourier transform on the image domain.\n",
        "        freq_repr = torch.abs(torch.fft.fft(latent, norm=\"ortho\"))\n",
        "        return self.net(freq_repr)\n",
        "\n",
        "class BiologicalPathway(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(BiologicalPathway, self).__init__()\n",
        "        # Focus on eye region artifacts; in practice, you may crop eye regions.\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        return self.net(latent)\n"
      ],
      "metadata": {
        "id": "UZILHMe1doon"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Refinement Layer"
      ],
      "metadata": {
        "id": "l_iqzD7YdtOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class ContrastiveRefinement(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(ContrastiveRefinement, self).__init__()\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        # Project latent representation into contrastive space.\n",
        "        return self.projector(latent)\n",
        "\n",
        "    def contrastive_loss(self, z1, z2):\n",
        "        # A simple L2 loss between projections for positive pairs.\n",
        "        return ((z1 - z2)**2).mean()\n"
      ],
      "metadata": {
        "id": "airSHDWJduVj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Equilibrium Component (NashAE-Inspired)"
      ],
      "metadata": {
        "id": "5pJhW3q8dx4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NashAE(nn.Module):\n",
        "    def __init__(self, in_dim=128, latent_dim=64):\n",
        "        super(NashAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, latent_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, in_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Discriminator to enforce a prior (e.g. sparse latent space)\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        z = self.encoder(latent)\n",
        "        rec = self.decoder(z)\n",
        "        d_score = self.discriminator(z)\n",
        "        return z, rec, d_score\n"
      ],
      "metadata": {
        "id": "C6LkIRiRdy_6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Fusion Mechanism"
      ],
      "metadata": {
        "id": "y8RzSrncd3BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecisionFusion(nn.Module):\n",
        "    def __init__(self, in_dims=[64, 64, 64, 64, 64], out_dim=2):\n",
        "        super(DecisionFusion, self).__init__()\n",
        "        total_dim = sum(in_dims)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(total_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, semantic, frequency, biological, contrastive, nash):\n",
        "        # Simulate dynamic weighting (here simply concatenating features)\n",
        "        # In practice, you could compute TAD metrics and use them as weights.\n",
        "        fused_features = torch.cat([semantic, frequency, biological, contrastive, nash], dim=1)\n",
        "        return self.fc(fused_features)\n"
      ],
      "metadata": {
        "id": "0o7SlY28d2nT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assemble the Full Hierarchical Disentanglement Ensemble (HDE)"
      ],
      "metadata": {
        "id": "LA2tEBk7d8EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HDE(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(HDE, self).__init__()\n",
        "        # First-level disentanglement via DR-GAN\n",
        "        self.drgan = DRGAN(latent_dim=latent_dim)\n",
        "\n",
        "        # Second-level analysis: three pathways (each output dimension = 64)\n",
        "        self.semantic = SemanticPathway(in_dim=latent_dim, out_dim=64)\n",
        "        self.frequency = FrequencyPathway(in_dim=latent_dim, out_dim=64)\n",
        "        self.biological = BiologicalPathway(in_dim=latent_dim, out_dim=64)\n",
        "\n",
        "        # Contrastive refinement\n",
        "        self.contrastive = ContrastiveRefinement(in_dim=latent_dim, out_dim=64)\n",
        "\n",
        "        # Adversarial equilibrium component (NashAE-inspired)\n",
        "        self.nash = NashAE(in_dim=latent_dim, latent_dim=64)\n",
        "\n",
        "        # Decision fusion: fuse five 64-dim features into classification logits (real vs fake)\n",
        "        self.fusion = DecisionFusion(in_dims=[64,64,64,64,64], out_dim=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # First-level: DR-GAN disentanglement\n",
        "        latent, rec, class_logits = self.drgan(x)\n",
        "\n",
        "        # Second-level: process latent through parallel pathways\n",
        "        semantic_feat  = self.semantic(latent)\n",
        "        frequency_feat = self.frequency(latent)\n",
        "        biological_feat = self.biological(latent)\n",
        "        contrastive_feat = self.contrastive(latent)\n",
        "        nash_z, nash_rec, nash_dscore = self.nash(latent)\n",
        "\n",
        "        # Fusion: combine all pathways dynamically\n",
        "        fused_logits = self.fusion(semantic_feat, frequency_feat, biological_feat, contrastive_feat, nash_z)\n",
        "\n",
        "        # The overall loss will combine:\n",
        "        # - Classification loss (from DR-GAN and fusion output)\n",
        "        # - Reconstruction losses (from DR-GAN and NashAE)\n",
        "        # - Contrastive loss (from the contrastive module)\n",
        "        # - Adversarial loss (from NashAE discriminator)\n",
        "        return {\n",
        "            'latent': latent,\n",
        "            'drgan_rec': rec,\n",
        "            'drgan_class': class_logits,\n",
        "            'semantic': semantic_feat,\n",
        "            'frequency': frequency_feat,\n",
        "            'biological': biological_feat,\n",
        "            'contrastive': contrastive_feat,\n",
        "            'nash_z': nash_z,\n",
        "            'nash_rec': nash_rec,\n",
        "            'nash_dscore': nash_dscore,\n",
        "            'fused_logits': fused_logits\n",
        "        }\n"
      ],
      "metadata": {
        "id": "GJZaVaYZd7w-"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation Routines"
      ],
      "metadata": {
        "id": "j5vdYUUZeBRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, dataloader, optimizer, device, criterion_cls, lambda_rec=1.0, lambda_adv=0.1, lambda_ctr=0.1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for imgs, labels in dataloader:\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        # Classification losses: from DR-GAN branch and fusion branch\n",
        "        loss_cls_drgan = criterion_cls(outputs['drgan_class'], labels)\n",
        "        loss_cls_fusion = criterion_cls(outputs['fused_logits'], labels)\n",
        "\n",
        "        # Reconstruction losses (L1 loss)\n",
        "        loss_rec_drgan = nn.L1Loss()(outputs['drgan_rec'], imgs)\n",
        "        loss_rec_nash = nn.L1Loss()(outputs['nash_rec'], outputs['latent'])\n",
        "\n",
        "        # Contrastive loss: here we simulate a contrastive loss by pairing the latent with itself\n",
        "        # In practice, you would construct contrastive pairs\n",
        "        loss_ctr = outputs['contrastive'].pow(2).mean()\n",
        "\n",
        "        # Adversarial loss for NashAE discriminator (binary cross entropy)\n",
        "        # Here we assume a target prior of 0.5 for all latent codes\n",
        "        adv_target = torch.full_like(outputs['nash_dscore'], 0.5, device=device)\n",
        "        loss_adv = nn.BCELoss()(outputs['nash_dscore'], adv_target)\n",
        "\n",
        "        # Total loss: weighted sum of all losses\n",
        "        loss = (loss_cls_drgan + loss_cls_fusion) + lambda_rec*(loss_rec_drgan + loss_rec_nash) + lambda_ctr*loss_ctr + lambda_adv*loss_adv\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def evaluate(model, dataloader, device, criterion_cls):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    losses = 0\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion_cls(outputs['fused_logits'], labels)\n",
        "            losses += loss.item()\n",
        "            preds = torch.argmax(outputs['fused_logits'], dim=1)\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "    # Compute metrics\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "    try:\n",
        "        auc = roc_auc_score(all_labels, all_preds)\n",
        "    except Exception:\n",
        "        auc = 0.0\n",
        "    return losses / len(dataloader), acc, prec, rec, f1, auc\n"
      ],
      "metadata": {
        "id": "2cdHbxSoeA_8"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "5EbRtCa-eGAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the HDE model\n",
        "latent_dim = 128\n",
        "model = HDE(latent_dim=latent_dim).to(device)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")  # Add this line to print epoch number\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device, criterion_cls,)\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1, val_auc = evaluate(model, val_loader, device, criterion_cls)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3161QuAeIXr",
        "outputId": "2d206b38-12b6-4a41-9720-84c0c43e819e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 206MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10:\n",
            "  Train Loss: 1.4846\n",
            "  Val Loss: 0.3982, Acc: 0.8184, Prec: 0.8236, Rec: 0.8184, F1: 0.8176, AUC: 0.8183\n",
            "Epoch 2/10:\n",
            "  Train Loss: 1.2895\n",
            "  Val Loss: 0.3640, Acc: 0.8389, Prec: 0.8441, Rec: 0.8389, F1: 0.8383, AUC: 0.8389\n",
            "Epoch 3/10:\n",
            "  Train Loss: 1.2155\n",
            "  Val Loss: 0.3300, Acc: 0.8545, Prec: 0.8558, Rec: 0.8545, F1: 0.8543, AUC: 0.8544\n",
            "Epoch 4/10:\n",
            "  Train Loss: 1.1677\n",
            "  Val Loss: 0.3668, Acc: 0.8343, Prec: 0.8527, Rec: 0.8343, F1: 0.8321, AUC: 0.8343\n",
            "Epoch 5/10:\n",
            "  Train Loss: 1.1388\n",
            "  Val Loss: 0.3042, Acc: 0.8689, Prec: 0.8710, Rec: 0.8689, F1: 0.8687, AUC: 0.8689\n",
            "Epoch 6/10:\n",
            "  Train Loss: 1.1015\n",
            "  Val Loss: 0.3172, Acc: 0.8631, Prec: 0.8725, Rec: 0.8631, F1: 0.8622, AUC: 0.8631\n",
            "Epoch 7/10:\n",
            "  Train Loss: 1.0881\n",
            "  Val Loss: 0.2729, Acc: 0.8831, Prec: 0.8834, Rec: 0.8831, F1: 0.8830, AUC: 0.8831\n",
            "Epoch 8/10:\n",
            "  Train Loss: 1.0509\n",
            "  Val Loss: 0.2880, Acc: 0.8757, Prec: 0.8801, Rec: 0.8757, F1: 0.8753, AUC: 0.8757\n",
            "Epoch 9/10:\n",
            "  Train Loss: 1.0467\n",
            "  Val Loss: 0.2549, Acc: 0.8918, Prec: 0.8936, Rec: 0.8918, F1: 0.8916, AUC: 0.8918\n",
            "Epoch 10/10:\n",
            "  Train Loss: 1.0043\n",
            "  Val Loss: 0.2474, Acc: 0.8965, Prec: 0.8989, Rec: 0.8965, F1: 0.8963, AUC: 0.8965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Saving the Model"
      ],
      "metadata": {
        "id": "xZtBA2DyjYDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_prec, test_rec, test_f1, test_auc = evaluate(model, test_loader, device, criterion_cls)\n",
        "print(\"Test Metrics:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  Precision: {test_prec:.4f}\")\n",
        "print(f\"  Recall: {test_rec:.4f}\")\n",
        "print(f\"  F1-score: {test_f1:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'hde_model.pth')\n",
        "print(\"Model saved as hde_model.pth\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AIiWKXAjZMq",
        "outputId": "4c31b215-fb7c-4ae9-d103-f77b05358170"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "  Loss: 0.2510\n",
            "  Accuracy: 0.8951\n",
            "  Precision: 0.8976\n",
            "  Recall: 0.8951\n",
            "  F1-score: 0.8949\n",
            "  AUC: 0.8951\n",
            "Model saved as hde_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "IMAGE_SIZE = 256  # Adjust if necessary\n",
        "\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "test_data = datasets.ImageFolder('/content/dataset/real_vs_fake/real-vs-fake/test', transform=val_test_transforms)\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "#  model class is named 'HDE'\n",
        "latent_dim = 128\n",
        "model = HDE(latent_dim=latent_dim)\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model.load_state_dict(torch.load('hde_model.pth'))\n",
        "\n",
        "# Move the model to the appropriate device (GPU if available)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "       model.eval()\n",
        "       all_preds = []\n",
        "       all_labels = []\n",
        "       with torch.no_grad():\n",
        "           for imgs, labels in dataloader:\n",
        "               imgs = imgs.to(device)\n",
        "               labels = labels.to(device)\n",
        "               outputs = model(imgs)\n",
        "               preds = torch.argmax(outputs['fused_logits'], dim=1)\n",
        "               all_preds.extend(preds.cpu().numpy())\n",
        "               all_labels.extend(labels.cpu().numpy())\n",
        "       # Compute metrics\n",
        "       acc = accuracy_score(all_labels, all_preds)\n",
        "       prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       try:\n",
        "           auc = roc_auc_score(all_labels, all_preds)\n",
        "       except Exception:\n",
        "           auc = 0.0\n",
        "       return acc, prec, rec, f1, auc\n",
        "\n",
        "test_acc, test_prec, test_rec, test_f1, test_auc = evaluate(model, test_loader, device)\n",
        "print(\"Test Metrics:\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  Precision: {test_prec:.4f}\")\n",
        "print(f\"  Recall: {test_rec:.4f}\")\n",
        "print(f\"  F1-score: {test_f1:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")"
      ],
      "metadata": {
        "id": "D8fde4eQEvQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def display_qualitative_results(model, dataloader, device, class_names, num_images=10):\n",
        "    model.eval()\n",
        "    images_shown = 2\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(num_images*3, 3))\n",
        "\n",
        "    # If only one image, axs is not a list, so we make it one.\n",
        "    if num_images == 4:\n",
        "        axs = [axs]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            outputs = model(imgs)\n",
        "            fused_logits = outputs['fused_logits']\n",
        "            probs = torch.softmax(fused_logits, dim=1)\n",
        "            preds = torch.argmax(fused_logits, dim=1)\n",
        "\n",
        "            for i in range(imgs.size(0)):\n",
        "                if images_shown >= num_images:\n",
        "                    break\n",
        "\n",
        "                # Denormalize: inverse normalization assuming mean=0.5 and std=0.5 for each channel.\n",
        "                img = imgs[i].cpu() * 0.5 + 0.5\n",
        "                img = img.permute(1, 2, 0).numpy()\n",
        "\n",
        "                true_label = class_names[labels[i].item()]\n",
        "                pred_label = class_names[preds[i].item()]\n",
        "                confidence = probs[i][preds[i].item()].item()\n",
        "\n",
        "                title = f\"T: {true_label}\\nP: {pred_label}\\nConf: {confidence:.2f}\"\n",
        "\n",
        "                axs[images_shown].imshow(img)\n",
        "                axs[images_shown].axis('off')\n",
        "                axs[images_shown].set_title(title, fontsize=10)\n",
        "\n",
        "                images_shown += 1\n",
        "            if images_shown >= num_images:\n",
        "                break\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Assuming your ImageFolder dataset has a 'classes' attribute, e.g., ['Real', 'Fake']\n",
        "class_names = train_data.classes  # or explicitly: class_names = ['Real', 'Fake']\n",
        "\n",
        "# Display qualitative results using the test set (change num_images as desired)\n",
        "display_qualitative_results(model, test_loader, device, class_names, num_images=10)\n"
      ],
      "metadata": {
        "id": "Fy3dy3W2RX4a"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}