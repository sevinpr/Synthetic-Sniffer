{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading dataset from kaggle"
      ],
      "metadata": {
        "id": "UfVFJ623aQ_Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StyleGAN & Flickr for training and validation"
      ],
      "metadata": {
        "id": "WGTiKxuEaxiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import shutil\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Specify the desired download path\n",
        "download_path = \"/content/dataset\"\n",
        "\n",
        "# Create the destination directory if it doesn't exist\n",
        "os.makedirs(download_path, exist_ok=True)\n",
        "\n",
        "# Download the dataset (initially downloaded to Kaggle cache)\n",
        "cache_path = kagglehub.dataset_download(\"xhlulu/140k-real-and-fake-faces\")\n",
        "print(\"Path to dataset files (cache):\", cache_path)  # 'cache_path' points to the cache\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xU-P3kEg6Rb",
        "outputId": "d0775be6-9abf-43f5-f462-591590ebac3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files (cache): /kaggle/input/140k-real-and-fake-faces\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define source (Kaggle cache) and destination paths\n",
        "source_path = '/kaggle/input/140k-real-and-fake-faces/real_vs_fake'  # Adjust if version changes\n",
        "destination_path = download_path\n",
        "\n",
        "# Iterate over all items in the source directory and copy them\n",
        "for item in os.listdir(source_path):\n",
        "    src_item = os.path.join(source_path, item)\n",
        "    dst_item = os.path.join(destination_path, item)\n",
        "\n",
        "    # If item is a directory, use shutil.copytree, else use shutil.copy2\n",
        "    if os.path.isdir(src_item):\n",
        "        shutil.copytree(src_item, dst_item)\n",
        "    else:\n",
        "        shutil.copy2(src_item, dst_item)  # copy2 preserves metadata\n",
        "\n",
        "print(f\"Files copied from '{source_path}' to '{destination_path}' successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGl1hlfYi-dC",
        "outputId": "643a5114-682e-4147-e558-7d0443318c19"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files copied from '/kaggle/input/140k-real-and-fake-faces/real_vs_fake' to '/content/dataset' successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PRO-GAN dataset for testing"
      ],
      "metadata": {
        "id": "TKyNwAsLatKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from torchvision import datasets\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"mayankjha146025/fake-face-images-generated-from-different-gans\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# Check if the dataset contains an 'images' folder\n",
        "# If not, adjust the folder name accordingly\n",
        "if not os.path.exists(os.path.join(path, 'images')):\n",
        "    # List all folders in the dataset path\n",
        "    folders = [f for f in os.listdir(path) if os.path.isdir(os.path.join(path, f))]\n",
        "    if folders:\n",
        "        # Assume the first folder in the list is the correct one\n",
        "        image_folder_path = os.path.join(path, folders[0])\n",
        "    else:\n",
        "        raise FileNotFoundError(\"No subfolders found in the dataset directory.\")\n",
        "else:\n",
        "    image_folder_path = os.path.join(path, 'images')\n",
        "\n",
        "# Create an ImageFolder dataset object to easily access the data\n",
        "dataset = datasets.ImageFolder(image_folder_path)\n",
        "\n",
        "# Print the total number of samples in the dataset\n",
        "print(\"Total number of data samples:\", len(dataset))"
      ],
      "metadata": {
        "id": "0LfBvm8-1zb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78d1c268-0aa0-4e0e-d7c9-549be49282d6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/fake-face-images-generated-from-different-gans\n",
            "Total number of data samples: 21300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Swap the testing folder to test with PROGAN"
      ],
      "metadata": {
        "id": "NYTscHL-a7N2"
      }
    },
    {
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "# 1. Delete content in /content/dataset/real_vs_fake/test/fake\n",
        "fake_folder = '/content/dataset/real_vs_fake/test/fake'\n",
        "for filename in os.listdir(fake_folder):\n",
        "    file_path = os.path.join(fake_folder, filename)\n",
        "    try:\n",
        "        if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "            os.unlink(file_path)\n",
        "        elif os.path.isdir(file_path):\n",
        "            shutil.rmtree(file_path)\n",
        "    except Exception as e:\n",
        "        print('Failed to delete %s. Reason: %s' % (file_path, e))\n",
        "\n",
        "# 2. Randomly delete 2100 images from /content/dataset/real_vs_fake/real-vs-fake/test/real\n",
        "real_folder = '/content/dataset/real_vs_fake/test/real'\n",
        "real_images = os.listdir(real_folder)\n",
        "images_to_delete = random.sample(real_images, 2900)\n",
        "for image in images_to_delete:\n",
        "    image_path = os.path.join(real_folder, image)\n",
        "    os.remove(image_path)\n",
        "\n",
        "# 3. Move images from /content/fake-face-images-generated-from-different-gans/GAN_Generated_Fake_Images/ProGAN_128x128 to /content/dataset/real_vs_fake/real-vs-fake/test/fake\n",
        "source_folder = '/kaggle/input/fake-face-images-generated-from-different-gans/GAN_Generated_Fake_Images/ProGAN_128x128'\n",
        "for filename in os.listdir(source_folder):\n",
        "    source_path = os.path.join(source_folder, filename)\n",
        "    destination_path = os.path.join(fake_folder, filename)\n",
        "    # Use shutil.copy instead of shutil.move\n",
        "    shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(\"Operations completed successfully.\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "eRLw5sw948lC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e610760-98d3-4aa0-9546-0a73fda79878"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Operations completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Augmentation"
      ],
      "metadata": {
        "id": "KnPMRa4gdWxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "\n",
        "# Check device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "er3cW9Drc-YI",
        "outputId": "30abc4f4-141d-44aa-ce1a-31e583de1bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_data_quantity(dataset, dataset_name):\n",
        "    print(f\"Dataset: {dataset_name}\")\n",
        "    print(f\"  Total samples: {len(dataset)}\")\n",
        "    # If the dataset is an instance of ImageFolder, it contains a 'targets' attribute.\n",
        "    if hasattr(dataset, 'targets'):\n",
        "        from collections import Counter\n",
        "        counts = Counter(dataset.targets)\n",
        "        # Map numeric labels back to class names using class_to_idx\n",
        "        idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
        "        print(\"  Class distribution:\")\n",
        "        for cls, count in counts.items():\n",
        "            print(f\"    {idx_to_class[cls]}: {count}\")\n",
        "    else:\n",
        "        print(\"  No target information available.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "R_6TcKhcHOWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (Keep previous imports and device check)\n",
        "import torch.nn.functional as F # For contrastive loss / cosine similarity\n",
        "import torch.fft # For Frequency Pathway\n",
        "\n",
        "# Define the target size (consistent with your model's FC layer)\n",
        "IMAGE_SIZE = 256\n",
        "\n",
        "# Define data transforms\n",
        "# Standard transforms for training\n",
        "train_transforms_standard = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1), # Less aggressive jitter for main view\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
        "])\n",
        "\n",
        "# Stronger augmentation for contrastive learning view\n",
        "train_transforms_contrastive = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15), # Slightly more rotation\n",
        "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.1, hue=0.1), # Stronger jitter\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.GaussianBlur(kernel_size=int(0.1 * IMAGE_SIZE) // 2 * 2 + 1, sigma=(0.1, 2.0)), # Add blur\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
        "])\n",
        "\n",
        "# Transforms for validation/testing (no augmentation)\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5] * 3, std=[0.5] * 3)\n",
        "])\n",
        "\n",
        "# --- Custom Dataset Wrapper for Contrastive Learning ---\n",
        "# We need two augmented views of each image for contrastive loss\n",
        "class ContrastiveImageFolder(datasets.ImageFolder):\n",
        "    def __init__(self, root, transform1, transform2, **kwargs):\n",
        "        super().__init__(root, transform=None, **kwargs) # Initial transform is None\n",
        "        self.transform1 = transform1\n",
        "        self.transform2 = transform2\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path, target = self.samples[index]\n",
        "        sample = self.loader(path)\n",
        "        # Apply the two different transforms\n",
        "        img1 = self.transform1(sample)\n",
        "        img2 = self.transform2(sample)\n",
        "        return img1, img2, target # Return two views and the label\n",
        "\n",
        "\n",
        "# --- Load or Create Datasets ---\n",
        "train_data_path = 'preprocessed_train_data_contrastive.pt' # Use new names\n",
        "val_data_path = 'preprocessed_val_data.pt'\n",
        "test_data_path = 'preprocessed_test_data.pt'\n",
        "\n",
        "if os.path.exists(train_data_path) and os.path.exists(val_data_path) and os.path.exists(test_data_path):\n",
        "    # Load preprocessed data (ensure these were saved correctly with contrastive setup if reloading)\n",
        "    # Note: Saving/Loading the custom ContrastiveImageFolder might need care.\n",
        "    # For simplicity, let's re-create datasets each time for this example.\n",
        "    print(\"Re-creating datasets for contrastive setup...\")\n",
        "    train_data = ContrastiveImageFolder('/content/dataset/real_vs_fake/train', transform1=train_transforms_standard, transform2=train_transforms_contrastive)\n",
        "    val_data = datasets.ImageFolder('/content/dataset/real_vs_fake/valid', transform=val_test_transforms)\n",
        "    test_data = datasets.ImageFolder('/content/dataset/real_vs_fake/test', transform=val_test_transforms)\n",
        "    # Consider saving/loading logic if dataset creation is slow\n",
        "else:\n",
        "    # Create and save preprocessed data\n",
        "    print(\"Creating datasets for contrastive setup...\")\n",
        "    train_data = ContrastiveImageFolder('/content/dataset/real_vs_fake/train', transform1=train_transforms_standard, transform2=train_transforms_contrastive)\n",
        "    val_data = datasets.ImageFolder('/content/dataset/real_vs_fake/valid', transform=val_test_transforms)\n",
        "    test_data = datasets.ImageFolder('/content/dataset/real_vs_fake/test', transform=val_test_transforms)\n",
        "\n",
        "    # Saving the ContrastiveImageFolder directly might not work as intended\n",
        "    # It's often better to save the raw data paths and apply transforms on the fly\n",
        "    # Or save the processed tensors directly if memory allows (less flexible)\n",
        "    # Skipping save for this example to ensure correct transforms are applied.\n",
        "    # torch.save(train_data, train_data_path)\n",
        "    # torch.save(val_data, val_data_path)\n",
        "    # torch.save(test_data, test_data_path)\n",
        "\n",
        "# --- Re-define the DataLoaders ---\n",
        "batch_size = 64 # Adjusted batch size maybe needed due to memory\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "# Check data quantity\n",
        "check_data_quantity(train_data, \"Train (Contrastive)\")\n",
        "check_data_quantity(val_data, \"Validation\")\n",
        "check_data_quantity(test_data, \"Test\")"
      ],
      "metadata": {
        "id": "Xkwlv1Ern-7J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c707651-ccec-43db-b6c7-05c8d492b765"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating datasets for contrastive setup...\n",
            "Dataset: Train (Contrastive)\n",
            "  Total samples: 100000\n",
            "  Class distribution:\n",
            "    fake: 50000\n",
            "    real: 50000\n",
            "Dataset: Validation\n",
            "  Total samples: 20000\n",
            "  Class distribution:\n",
            "    fake: 10000\n",
            "    real: 10000\n",
            "Dataset: Test\n",
            "  Total samples: 14200\n",
            "  Class distribution:\n",
            "    fake: 7100\n",
            "    real: 7100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the training dataset to check the shape after transforms\n",
        "print(\"Checking shape of a sample image pair after preprocessing:\")\n",
        "# Retrieve the first sample (two image tensors and label) from the train_data\n",
        "sample_image_tensor1, sample_image_tensor2, sample_label = train_data[0]\n",
        "\n",
        "print(f\"  Type of the preprocessed image view 1: {type(sample_image_tensor1)}\")\n",
        "print(f\"  Shape of the preprocessed image view 1 (C, H, W): {sample_image_tensor1.shape}\")\n",
        "print(f\"  Type of the preprocessed image view 2: {type(sample_image_tensor2)}\")\n",
        "print(f\"  Shape of the preprocessed image view 2 (C, H, W): {sample_image_tensor2.shape}\")\n",
        "print(f\"  Label of the sample: {sample_label} (Class: {train_data.classes[sample_label]})\")\n",
        "\n",
        "# Check the shape of a batch from the DataLoader\n",
        "print(\"\\nChecking shape of a batch from the DataLoader:\")\n",
        "# Get one batch of data from the train_loader\n",
        "images1_batch, images2_batch, labels_batch = next(iter(train_loader))\n",
        "print(f\"  Shape of image batch view 1 (B, C, H, W): {images1_batch.shape}\")\n",
        "print(f\"  Shape of image batch view 2 (B, C, H, W): {images2_batch.shape}\")\n",
        "print(f\"  Shape of a label batch (B): {labels_batch.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE6CGBBMlhgv",
        "outputId": "8911b519-3317-4754-81b2-f31bbf58d23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking shape of a sample image pair after preprocessing:\n",
            "  Type of the preprocessed image view 1: <class 'torch.Tensor'>\n",
            "  Shape of the preprocessed image view 1 (C, H, W): torch.Size([3, 256, 256])\n",
            "  Type of the preprocessed image view 2: <class 'torch.Tensor'>\n",
            "  Shape of the preprocessed image view 2 (C, H, W): torch.Size([3, 256, 256])\n",
            "  Label of the sample: 0 (Class: fake)\n",
            "\n",
            "Checking shape of a batch from the DataLoader:\n",
            "  Shape of image batch view 1 (B, C, H, W): torch.Size([64, 3, 256, 256])\n",
            "  Shape of image batch view 2 (B, C, H, W): torch.Size([64, 3, 256, 256])\n",
            "  Shape of a label batch (B): torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample from the test dataset to check the shape after transforms\n",
        "print(\"\\nChecking shape of a sample TEST image after preprocessing:\")\n",
        "# Retrieve the first sample (image tensor and its label) from the test_data\n",
        "sample_test_image_tensor, sample_test_label = test_data[0]\n",
        "\n",
        "# The transforms.ToTensor() step moves the channel dimension to the front (C, H, W)\n",
        "# The transforms.Normalize() step does not change the shape.\n",
        "print(f\"  Type of the preprocessed test image: {type(sample_test_image_tensor)}\")\n",
        "print(f\"  Shape of the preprocessed test image (Channels, Height, Width): {sample_test_image_tensor.shape}\")\n",
        "print(f\"  Label of the test sample: {sample_test_label} (Class: {test_data.classes[sample_test_label]})\") # Optional: see label\n",
        "\n",
        "# You can also check the shape of a batch from the test DataLoader\n",
        "print(\"\\nChecking shape of a batch from the TEST DataLoader:\")\n",
        "# Get one batch of data from the test_loader\n",
        "test_images_batch, test_labels_batch = next(iter(test_loader))\n",
        "print(f\"  Shape of a test image batch (Batch Size, Channels, Height, Width): {test_images_batch.shape}\")\n",
        "print(f\"  Shape of a test label batch (Batch Size): {test_labels_batch.shape}\") # Optional: see labels shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyXkbx1qnppv",
        "outputId": "3c852bb4-36cc-4b1e-de75-f82b7ac895fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checking shape of a sample TEST image after preprocessing:\n",
            "  Type of the preprocessed test image: <class 'torch.Tensor'>\n",
            "  Shape of the preprocessed test image (Channels, Height, Width): torch.Size([3, 256, 256])\n",
            "  Label of the test sample: 0 (Class: fake)\n",
            "\n",
            "Checking shape of a batch from the TEST DataLoader:\n",
            "  Shape of a test image batch (Batch Size, Channels, Height, Width): torch.Size([64, 3, 256, 256])\n",
            "  Shape of a test label batch (Batch Size): torch.Size([64])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First-Level Disentanglement: DR-GAN Module"
      ],
      "metadata": {
        "id": "32q4doxSdf0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "class DRGANEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGANEncoder, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # Simple CNN architecture;\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),  # 256->128\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),  # 128->64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),  # 64->32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),  # 32->16\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        self.fc = nn.Linear(512*16*16, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        latent = self.fc(x)\n",
        "        return latent\n",
        "\n",
        "class DRGANEncoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128, freeze_backbone=True):\n",
        "        super(DRGANEncoder, self).__init__()\n",
        "        # Example using ResNet50\n",
        "        backbone = models.resnet50(pretrained=True)\n",
        "        # Remove the original classifier\n",
        "        self.features = nn.Sequential(*list(backbone.children())[:-1])\n",
        "\n",
        "        if freeze_backbone:\n",
        "            for param in self.features.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Adjust the input dimension of the FC layer based on backbone output\n",
        "        # ResNet50 output feature dim is 2048\n",
        "        num_ftrs = backbone.fc.in_features # or simply 2048 for ResNet50\n",
        "        self.fc = nn.Linear(num_ftrs, latent_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1) # Flatten features\n",
        "        latent = self.fc(x)\n",
        "        return latent\n",
        "\n",
        "class DRGANDecoder(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGANDecoder, self).__init__()\n",
        "        self.fc = nn.Linear(latent_dim, 512*16*16)\n",
        "        self.deconv = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),  # 16->32\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),  # 32->64\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),   # 64->128\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),     # 128->256\n",
        "            nn.Tanh()  # Assuming image pixels in [-1, 1]\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        x = self.fc(latent)\n",
        "        x = x.view(x.size(0), 512, 16, 16)\n",
        "        rec = self.deconv(x)\n",
        "        return rec\n",
        "\n",
        "class DRGAN(nn.Module):\n",
        "    def __init__(self, latent_dim=128):\n",
        "        super(DRGAN, self).__init__()\n",
        "        self.encoder = DRGANEncoder(latent_dim=latent_dim)\n",
        "        self.decoder = DRGANDecoder(latent_dim=latent_dim)\n",
        "        # A simple classifier to differentiate real vs. fake based on latent code\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(64, 2)  # 2 classes: real, fake\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.encoder(x)\n",
        "        rec = self.decoder(latent)\n",
        "        class_logits = self.classifier(latent)\n",
        "        return latent, rec, class_logits\n"
      ],
      "metadata": {
        "id": "djTfRmr-dhRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Second-Level Analysis: Multi-Pathway Modules"
      ],
      "metadata": {
        "id": "RYcu4RKEdlb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SemanticPathway(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(SemanticPathway, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        return self.net(latent)\n",
        "\n",
        "class FrequencyPathway(nn.Module):\n",
        "    def __init__(self, image_size=256, out_dim=64):\n",
        "        super(FrequencyPathway, self).__init__()\n",
        "        # Process frequency domain of the IMAGE\n",
        "        # Calculate flattened size after FFT magnitude and pooling\n",
        "        # FFT magnitude shape: (B, C, H, W) or (B, C, H, W//2 + 1) for rfft\n",
        "        # averaging over channels and apply adaptive pooling\n",
        "        self.pool = nn.AdaptiveAvgPool2d((8, 8)) # Pool to a fixed size ( 8x8)\n",
        "        fft_flat_dim = 8 * 8 # Size after pooling\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(fft_flat_dim, 128), # Intermediate layer\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(128, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, image):\n",
        "        # Input image shape: (B, C, H, W)\n",
        "        # Compute 2D real FFT\n",
        "        fft_coeffs = torch.fft.rfft2(image, norm=\"ortho\")\n",
        "        # Compute magnitude (abs)\n",
        "        fft_mag = torch.abs(fft_coeffs) # Shape: (B, C, H, W//2 + 1)\n",
        "\n",
        "        # Average magnitude across channels\n",
        "        fft_mag_pooled_channel = fft_mag.mean(dim=1, keepdim=True) # Shape: (B, 1, H, W//2 + 1)\n",
        "\n",
        "        # Apply adaptive average pooling to reduce spatial dimensions\n",
        "        pooled_fft_mag = self.pool(fft_mag_pooled_channel) # Shape: (B, 1, 8, 8)\n",
        "\n",
        "        # Flatten and pass through MLP\n",
        "        flat_fft = torch.flatten(pooled_fft_mag, 1) # Shape: (B, 64)\n",
        "        return self.net(flat_fft)\n",
        "\n",
        "class BiologicalPathway(nn.Module):\n",
        "     # NOTE: This pathway is currently simplified. A more accurate implementation\n",
        "     # would involve spatially-aware processing, potentially focusing on specific\n",
        "     # facial regions (like eyes) derived either from the image or intermediate\n",
        "     # feature maps, which significantly increases complexity.\n",
        "     # Here, it acts as another MLP processing the global latent code.\n",
        "    def __init__(self, in_dim=128, out_dim=64):\n",
        "        super(BiologicalPathway, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, out_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(out_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        return self.net(latent)\n"
      ],
      "metadata": {
        "id": "UZILHMe1doon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contrastive Refinement Layer"
      ],
      "metadata": {
        "id": "l_iqzD7YdtOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContrastiveRefinement(nn.Module):\n",
        "    def __init__(self, in_dim=128, out_dim=64): # out_dim is the projection dimension\n",
        "        super(ContrastiveRefinement, self).__init__()\n",
        "        # Projector head for contrastive learning (e.g., MLP)\n",
        "        self.projector = nn.Sequential(\n",
        "            nn.Linear(in_dim, in_dim // 2), # Intermediate layer\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_dim // 2, out_dim)\n",
        "            # No normalization here, will do in loss calculation\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        # Project latent representation into contrastive space.\n",
        "        projection = self.projector(latent)\n",
        "        return projection\n",
        "\n",
        "    # Contrastive loss is calculated in the training loop now\n",
        "    # def contrastive_loss(self, z1, z2): # Remove old loss func\n",
        "    #     # A simple L2 loss between projections for positive pairs.\n",
        "    #     return ((z1 - z2)**2).mean()"
      ],
      "metadata": {
        "id": "airSHDWJduVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adversarial Equilibrium Component (NashAE-Inspired)"
      ],
      "metadata": {
        "id": "5pJhW3q8dx4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class NashAE(nn.Module):\n",
        "    def __init__(self, in_dim=128, latent_dim=64):\n",
        "        super(NashAE, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(in_dim, latent_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(latent_dim, in_dim),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        # Discriminator to enforce a prior (e.g. sparse latent space)\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(latent_dim, 32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(32, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        z = self.encoder(latent)\n",
        "        rec = self.decoder(z)\n",
        "        d_score = self.discriminator(z)\n",
        "        return z, rec, d_score\n"
      ],
      "metadata": {
        "id": "C6LkIRiRdy_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Fusion Mechanism"
      ],
      "metadata": {
        "id": "y8RzSrncd3BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecisionFusion(nn.Module):\n",
        "    def __init__(self, in_dims=[64, 64, 64, 64, 64], hidden_dim=64, out_dim=2):\n",
        "        super(DecisionFusion, self).__init__()\n",
        "        num_pathways = len(in_dims)\n",
        "        total_in_dim = sum(in_dims)\n",
        "\n",
        "        # Learnable weights for each pathway\n",
        "        self.pathway_weights = nn.Parameter(torch.ones(num_pathways))\n",
        "\n",
        "        # Fusion MLP\n",
        "        self.fc = nn.Sequential(\n",
        "            # Input dimension depends on how weights are applied\n",
        "            # Option 1: Weighted sum (requires all in_dims to be same) -> Simpler if possible\n",
        "            # Option 2: Weighted concatenation -> More general\n",
        "            # Let's use weighted concatenation here\n",
        "            nn.Linear(total_in_dim, hidden_dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(hidden_dim, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, semantic, frequency, biological, contrastive, nash):\n",
        "        features = [semantic, frequency, biological, contrastive, nash]\n",
        "        batch_size = features[0].size(0)\n",
        "\n",
        "        # Apply softmax to weights to get normalized attention scores\n",
        "        normalized_weights = F.softmax(self.pathway_weights, dim=0)\n",
        "\n",
        "        # Apply weights (element-wise multiplication, needs broadcasting)\n",
        "        weighted_features = []\n",
        "        for i, feat in enumerate(features):\n",
        "            # Reshape weight for broadcasting: (1, 1) -> (1, num_pathways) -> select i -> (1,) -> (1, 1) -> (B, 1)\n",
        "            weight = normalized_weights[i].view(1, 1).expand(batch_size, 1)\n",
        "            weighted_features.append(feat * weight.expand_as(feat)) # Expand weight to match feature dim\n",
        "\n",
        "        # Concatenate weighted features\n",
        "        fused_features = torch.cat(weighted_features, dim=1)\n",
        "\n",
        "        # Final classification\n",
        "        return self.fc(fused_features)\n",
        "\n",
        "    def get_weights(self):\n",
        "        # Helper to check learned weights\n",
        "        return F.softmax(self.pathway_weights, dim=0)"
      ],
      "metadata": {
        "id": "0o7SlY28d2nT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assemble the Full Hierarchical Disentanglement Ensemble (HDE)"
      ],
      "metadata": {
        "id": "LA2tEBk7d8EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HDE(nn.Module):\n",
        "    def __init__(self, latent_dim=128, image_size=256): # Added image_size\n",
        "        super(HDE, self).__init__()\n",
        "        # First-level disentanglement via DR-GAN\n",
        "        self.drgan = DRGAN(latent_dim=latent_dim)\n",
        "\n",
        "        # Second-level analysis: pathways\n",
        "        self.semantic = SemanticPathway(in_dim=latent_dim, out_dim=64)\n",
        "        # Frequency pathway now needs image size info and operates on image\n",
        "        self.frequency = FrequencyPathway(image_size=image_size, out_dim=64)\n",
        "        self.biological = BiologicalPathway(in_dim=latent_dim, out_dim=64)\n",
        "\n",
        "        # Contrastive refinement (projection head)\n",
        "        self.contrastive = ContrastiveRefinement(in_dim=latent_dim, out_dim=64) # Projector output dim = 64\n",
        "\n",
        "        # Adversarial equilibrium component (NashAE-inspired)\n",
        "        self.nash = NashAE(in_dim=latent_dim, latent_dim=64)\n",
        "\n",
        "        # Decision fusion: fuse five 64-dim features\n",
        "        self.fusion = DecisionFusion(in_dims=[64, 64, 64, 64, 64], out_dim=2)\n",
        "\n",
        "    # Forward pass now needs the original image 'x' for the frequency pathway\n",
        "    # It also needs the augmented view 'x_aug' if contrastive loss is computed here\n",
        "    # Let's compute contrastive projection separately in the training loop for clarity\n",
        "    def forward(self, x):\n",
        "        # First-level: DR-GAN disentanglement\n",
        "        latent, rec, class_logits = self.drgan(x) # DR-GAN runs on the primary view 'x'\n",
        "\n",
        "        # Second-level: process latent and image through parallel pathways\n",
        "        semantic_feat = self.semantic(latent)\n",
        "        frequency_feat = self.frequency(x) # Frequency pathway processes the image 'x'\n",
        "        biological_feat = self.biological(latent) # Simplified bio pathway on latent\n",
        "\n",
        "        # Contrastive projection (applied to latent from primary view 'x')\n",
        "        contrastive_feat = self.contrastive(latent)\n",
        "\n",
        "        # NashAE processing (applied to latent from primary view 'x')\n",
        "        nash_z, nash_rec, nash_dscore = self.nash(latent)\n",
        "\n",
        "        # Fusion: combine all pathways\n",
        "        # Note: contrastive_feat is used for contrastive loss, nash_z is from NashAE encoder\n",
        "        fused_logits = self.fusion(semantic_feat, frequency_feat, biological_feat, contrastive_feat, nash_z)\n",
        "\n",
        "        # Return dictionary of relevant outputs\n",
        "        return {\n",
        "            'latent': latent,\n",
        "            'drgan_rec': rec,\n",
        "            'drgan_class': class_logits,\n",
        "            'semantic': semantic_feat,\n",
        "            'frequency': frequency_feat,\n",
        "            'biological': biological_feat,\n",
        "            'contrastive': contrastive_feat, # Projection for contrastive loss\n",
        "            'nash_z': nash_z,\n",
        "            'nash_rec': nash_rec,\n",
        "            'nash_dscore': nash_dscore,\n",
        "            'fused_logits': fused_logits # Final classification output\n",
        "        }\n",
        "\n",
        "    # Helper function to get contrastive projection for the augmented view\n",
        "    def get_contrastive_projection(self, x_aug):\n",
        "         with torch.no_grad(): # Usually only encoder is needed, avoid full backprop through DR-GAN again if possible\n",
        "             latent_aug = self.drgan.encoder(x_aug)\n",
        "         contrastive_feat_aug = self.contrastive(latent_aug)\n",
        "         return contrastive_feat_aug"
      ],
      "metadata": {
        "id": "GJZaVaYZd7w-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training and Evaluation Routines"
      ],
      "metadata": {
        "id": "j5vdYUUZeBRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- START OF CELL BLOCK: Training and Evaluation Routines ---\n",
        "\n",
        "import time\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# --- Contrastive Loss Function (InfoNCE / NT-Xent) ---\n",
        "def info_nce_loss(features, temperature=0.1, device='cuda', eps=1e-7):\n",
        "    \"\"\"\n",
        "    Calculates the InfoNCE loss for contrastive learning.\n",
        "    Args:\n",
        "        features: Tensor of shape (2 * batch_size, proj_dim).\n",
        "                  Assumes the first half are projections from view 1 (z_i)\n",
        "                  and the second half are projections from view 2 (z_j).\n",
        "        temperature: Temperature scaling factor.\n",
        "        device: CUDA or CPU.\n",
        "        eps: Small epsilon for numerical stability.\n",
        "    Returns:\n",
        "        Loss tensor.\n",
        "    \"\"\"\n",
        "    if features.shape[0] % 2 != 0:\n",
        "        raise ValueError(\"Feature batch size must be even (view1 + view2).\")\n",
        "\n",
        "    batch_size = features.shape[0] // 2\n",
        "    if batch_size == 0:\n",
        "        return torch.tensor(0.0, device=device, requires_grad=True) # Return zero loss if batch is empty\n",
        "\n",
        "    # Check for NaNs/Infs in input features BEFORE normalization\n",
        "    if torch.isnan(features).any() or torch.isinf(features).any():\n",
        "        print(\"WARNING: NaN/Inf detected in features input to info_nce_loss!\")\n",
        "        # Option 1: Return a high loss or zero loss with no grad to avoid crash\n",
        "        # return torch.tensor(10.0, device=device, requires_grad=True)\n",
        "        # Option 2: Raise error\n",
        "        raise RuntimeError(\"NaN/Inf detected in info_nce_loss input features\")\n",
        "\n",
        "\n",
        "    # Normalize features - adding epsilon to norm calculation for stability\n",
        "    # norm = torch.norm(features, p=2, dim=1, keepdim=True)\n",
        "    # features_normalized = features / (norm + eps)\n",
        "    # Using F.normalize which should handle zero vectors internally\n",
        "    features_normalized = F.normalize(features, dim=1, eps=eps)\n",
        "\n",
        "    # Check for NaNs/Infs AFTER normalization\n",
        "    if torch.isnan(features_normalized).any() or torch.isinf(features_normalized).any():\n",
        "        print(\"WARNING: NaN/Inf detected in features AFTER normalization in info_nce_loss!\")\n",
        "        # Consider printing the norm values of the original features here if this happens\n",
        "        # print(\"Original feature norms:\", torch.norm(features, p=2, dim=1))\n",
        "        raise RuntimeError(\"NaN/Inf detected after normalization in info_nce_loss\")\n",
        "\n",
        "\n",
        "    # Compute cosine similarity matrix\n",
        "    similarity_matrix = torch.matmul(features_normalized, features_normalized.T) # Shape: (2B, 2B)\n",
        "\n",
        "    # Check similarity matrix\n",
        "    if torch.isnan(similarity_matrix).any() or torch.isinf(similarity_matrix).any():\n",
        "         print(\"WARNING: NaN/Inf detected in similarity_matrix in info_nce_loss!\")\n",
        "         raise RuntimeError(\"NaN/Inf detected in similarity_matrix\")\n",
        "\n",
        "\n",
        "    # --- Mask out diagonal (self-similarity) ---\n",
        "    mask = torch.eye(batch_size * 2, dtype=torch.bool).to(device)\n",
        "    # Select elements NOT on the diagonal\n",
        "    similarity_matrix_masked = similarity_matrix[~mask].view(batch_size * 2, -1) # Shape: (2B, 2B-1)\n",
        "\n",
        "\n",
        "    # --- Select positive pairs ---\n",
        "    # Get the projections for view 1 and view 2\n",
        "    z_i = features_normalized[:batch_size] # Shape: (B, D)\n",
        "    z_j = features_normalized[batch_size:] # Shape: (B, D)\n",
        "    # Calculate cosine similarity for positive pairs (i, j) - already normalized so just dot product\n",
        "    sim_ij = torch.sum(z_i * z_j, dim=1) # Shape: (B,)\n",
        "\n",
        "    # Positive similarities for anchor i vs positive j, and anchor j vs positive i\n",
        "    # Both should be the same value: sim(i, j) == sim(j, i)\n",
        "    positive_sim = torch.cat([sim_ij, sim_ij], dim=0) # Shape: (2B,)\n",
        "\n",
        "    # --- Combine positive with negatives for logits ---\n",
        "    # Logits for CrossEntropy: [positive_sim_for_anchor, neg_sim_1, neg_sim_2, ...]\n",
        "    # The `similarity_matrix_masked` contains all negative pairs for each anchor already.\n",
        "    # Prepend the positive similarity for each anchor.\n",
        "    logits = torch.cat([positive_sim.unsqueeze(1), similarity_matrix_masked], dim=1) # Shape: (2B, 2B)\n",
        "    logits /= temperature\n",
        "\n",
        "    # Check logits\n",
        "    if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
        "         print(\"WARNING: NaN/Inf detected in final logits in info_nce_loss!\")\n",
        "         # print(\"Max logit:\", logits[~torch.isinf(logits)].max())\n",
        "         # print(\"Min logit:\", logits[~torch.isinf(logits)].min())\n",
        "         # print(\"Temperature:\", temperature)\n",
        "         raise RuntimeError(\"NaN/Inf detected in logits\")\n",
        "\n",
        "\n",
        "    # Labels for CrossEntropyLoss are all zeros because the positive pair is constructed at index 0\n",
        "    ce_labels = torch.zeros(logits.shape[0], dtype=torch.long).to(device)\n",
        "\n",
        "    # Calculate CrossEntropyLoss\n",
        "    loss = F.cross_entropy(logits, ce_labels)\n",
        "    return loss\n",
        "\n",
        "\n",
        "# --- Training Function (One Epoch) ---\n",
        "def train_epoch(model, dataloader, optimizer, device, criterion_cls,\n",
        "                lambda_rec=1.0, lambda_adv=0.1, lambda_ctr=0.5, contrastive_temp=0.1,\n",
        "                gradient_clip_norm=1.0): # Added gradient clipping param\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_loss_cls_drgan, total_loss_cls_fusion = 0, 0\n",
        "    total_loss_rec_drgan, total_loss_rec_nash = 0, 0\n",
        "    total_loss_ctr, total_loss_adv = 0, 0\n",
        "    start_time = time.time()\n",
        "\n",
        "    batch_print_freq = 100 # How often to print batch progress\n",
        "\n",
        "    for batch_idx, (imgs1, imgs2, labels) in enumerate(dataloader):\n",
        "        imgs1 = imgs1.to(device) # Standard view\n",
        "        imgs2 = imgs2.to(device) # Augmented view for contrastive\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # --- Label Check (Keep this) ---\n",
        "        if not ((labels >= 0) & (labels < 2)).all():\n",
        "            print(f\"!!! ERROR: Invalid labels found in batch {batch_idx} !!!\")\n",
        "            print(f\"Unique labels in batch: {torch.unique(labels)}\")\n",
        "            raise ValueError(\"Invalid labels detected.\")\n",
        "        # --- End Label Check ---\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        try: # Add try-except around forward and loss calculation for better error isolation\n",
        "            # --- Forward pass for main view (imgs1) ---\n",
        "            outputs = model(imgs1)\n",
        "\n",
        "            # --- Get contrastive projection for augmented view (imgs2) ---\n",
        "            contrastive_feat_aug = model.get_contrastive_projection(imgs2)\n",
        "            contrastive_feat_main = outputs['contrastive']\n",
        "            all_contrastive_features = torch.cat([contrastive_feat_main, contrastive_feat_aug], dim=0)\n",
        "\n",
        "            # --- !!! ADD NaN/Inf CHECKS BEFORE EACH LOSS !!! ---\n",
        "\n",
        "            # Check inputs for Classification Losses\n",
        "            if torch.isnan(outputs['drgan_class']).any() or torch.isinf(outputs['drgan_class']).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'drgan_class' at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            if torch.isnan(outputs['fused_logits']).any() or torch.isinf(outputs['fused_logits']).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'fused_logits' at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            loss_cls_drgan = criterion_cls(outputs['drgan_class'], labels)\n",
        "            loss_cls_fusion = criterion_cls(outputs['fused_logits'], labels)\n",
        "\n",
        "            # Check inputs for Reconstruction Losses\n",
        "            if torch.isnan(outputs['drgan_rec']).any() or torch.isinf(outputs['drgan_rec']).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'drgan_rec' at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            if torch.isnan(imgs1).any() or torch.isinf(imgs1).any():\n",
        "                 print(f\"NaN/Inf DETECTED in input 'imgs1' at batch {batch_idx}!\")\n",
        "                 raise RuntimeError(\"NaN/Inf detected\")\n",
        "            loss_rec_drgan = F.l1_loss(outputs['drgan_rec'], imgs1)\n",
        "\n",
        "            if torch.isnan(outputs['nash_rec']).any() or torch.isinf(outputs['nash_rec']).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'nash_rec' at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            if torch.isnan(outputs['latent']).any() or torch.isinf(outputs['latent']).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'latent' at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            loss_rec_nash = F.l1_loss(outputs['nash_rec'], outputs['latent'])\n",
        "\n",
        "            # Check inputs for Contrastive Loss\n",
        "            # (NaN check moved inside info_nce_loss, but can keep one here too)\n",
        "            if torch.isnan(all_contrastive_features).any() or torch.isinf(all_contrastive_features).any():\n",
        "                print(f\"NaN/Inf DETECTED in 'all_contrastive_features' input at batch {batch_idx}!\")\n",
        "                raise RuntimeError(\"NaN/Inf detected\")\n",
        "            loss_ctr = info_nce_loss(all_contrastive_features, temperature=contrastive_temp, device=device)\n",
        "\n",
        "            # Check inputs for Adversarial Loss\n",
        "            if torch.isnan(outputs['nash_dscore']).any() or torch.isinf(outputs['nash_dscore']).any():\n",
        "                 print(f\"NaN/Inf DETECTED in 'nash_dscore' at batch {batch_idx}!\")\n",
        "                 raise RuntimeError(\"NaN/Inf detected\")\n",
        "            # Add epsilon clipping for BCE stability\n",
        "            nash_dscore_clipped = torch.clamp(outputs['nash_dscore'], min=1e-7, max=1.0 - 1e-7)\n",
        "            adv_target = torch.full_like(nash_dscore_clipped, 0.5, device=device)\n",
        "            loss_adv = F.binary_cross_entropy(nash_dscore_clipped, adv_target)\n",
        "\n",
        "            # --- !!! END NaN/Inf CHECKS !!! ---\n",
        "\n",
        "            # Check if individual losses are NaN/Inf\n",
        "            if torch.isnan(loss_cls_drgan): print(f\"NaN DETECTED in loss_cls_drgan batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_cls_drgan): print(f\"Inf DETECTED in loss_cls_drgan batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "            if torch.isnan(loss_cls_fusion): print(f\"NaN DETECTED in loss_cls_fusion batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_cls_fusion): print(f\"Inf DETECTED in loss_cls_fusion batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "            if torch.isnan(loss_rec_drgan): print(f\"NaN DETECTED in loss_rec_drgan batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_rec_drgan): print(f\"Inf DETECTED in loss_rec_drgan batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "            if torch.isnan(loss_rec_nash): print(f\"NaN DETECTED in loss_rec_nash batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_rec_nash): print(f\"Inf DETECTED in loss_rec_nash batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "            if torch.isnan(loss_ctr): print(f\"NaN DETECTED in loss_ctr batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_ctr): print(f\"Inf DETECTED in loss_ctr batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "            if torch.isnan(loss_adv): print(f\"NaN DETECTED in loss_adv batch {batch_idx}\"); raise RuntimeError(\"NaN Loss\")\n",
        "            if torch.isinf(loss_adv): print(f\"Inf DETECTED in loss_adv batch {batch_idx}\"); raise RuntimeError(\"Inf Loss\")\n",
        "\n",
        "\n",
        "            # --- Total Weighted Loss ---\n",
        "            loss_cls = loss_cls_drgan + loss_cls_fusion\n",
        "            loss_rec = loss_rec_drgan + loss_rec_nash\n",
        "            loss = loss_cls + lambda_rec * loss_rec + lambda_ctr * loss_ctr + lambda_adv * loss_adv\n",
        "\n",
        "            # Check final loss\n",
        "            if torch.isnan(loss) or torch.isinf(loss):\n",
        "                 print(f\"NaN/Inf DETECTED in final weighted 'loss' at batch {batch_idx}!\")\n",
        "                 # Print individual components again to see which one contributed\n",
        "                 print(f\"  Components -> cls:{loss_cls.item():.4f}, rec:{loss_rec.item():.4f}, ctr:{loss_ctr.item():.4f}, adv:{loss_adv.item():.4f}\")\n",
        "                 raise RuntimeError(\"NaN/Inf detected in final loss\")\n",
        "\n",
        "        except Exception as e:\n",
        "             print(f\"!!!!! Error during forward/loss calculation at batch {batch_idx} !!!!!\")\n",
        "             print(e)\n",
        "             # Optionally: save problematic batch for offline debugging\n",
        "             # torch.save({'imgs1': imgs1.cpu(), 'imgs2': imgs2.cpu(), 'labels': labels.cpu()}, f'error_batch_{batch_idx}.pt')\n",
        "             # print(f\"Saved problematic batch data to error_batch_{batch_idx}.pt\")\n",
        "             raise e # Re-raise the exception to stop training\n",
        "\n",
        "\n",
        "        # --- Backpropagation and Optimization ---\n",
        "        loss.backward()\n",
        "\n",
        "        # --- Gradient Clipping ( uncommented ) ---\n",
        "        if gradient_clip_norm is not None and gradient_clip_norm > 0:\n",
        "             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=gradient_clip_norm)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # --- Accumulate losses for logging ---\n",
        "        total_loss += loss.item()\n",
        "        total_loss_cls_drgan += loss_cls_drgan.item()\n",
        "        total_loss_cls_fusion += loss_cls_fusion.item()\n",
        "        total_loss_rec_drgan += loss_rec_drgan.item()\n",
        "        total_loss_rec_nash += loss_rec_nash.item()\n",
        "        total_loss_ctr += loss_ctr.item()\n",
        "        total_loss_adv += loss_adv.item()\n",
        "\n",
        "        # Optional: Print progress within epoch\n",
        "        if (batch_idx + 1) % batch_print_freq == 0:\n",
        "            current_avg_loss = total_loss / (batch_idx + 1)\n",
        "            print(f'    Batch {batch_idx+1}/{len(dataloader)}, Current Avg Loss: {current_avg_loss:.4f}')\n",
        "\n",
        "    # --- Epoch End Logging ---\n",
        "    end_time = time.time()\n",
        "    epoch_duration = end_time - start_time\n",
        "    num_batches = len(dataloader)\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    print(f\"    Avg Train Loss: {avg_loss:.4f} (Duration: {epoch_duration:.2f}s)\")\n",
        "    # More detailed breakdown\n",
        "    avg_cls_drgan = total_loss_cls_drgan / num_batches\n",
        "    avg_cls_fusion = total_loss_cls_fusion / num_batches\n",
        "    avg_rec_drgan = total_loss_rec_drgan / num_batches\n",
        "    avg_rec_nash = total_loss_rec_nash / num_batches\n",
        "    avg_ctr = total_loss_ctr / num_batches\n",
        "    avg_adv = total_loss_adv / num_batches\n",
        "\n",
        "    print(f\"      Breakdown (Avg): CLS[Drgan:{avg_cls_drgan:.4f}, Fusion:{avg_cls_fusion:.4f}] REC[Drgan:{avg_rec_drgan:.4f}, Nash:{avg_rec_nash:.4f}] CTR:{avg_ctr:.4f} ADV:{avg_adv:.4f}\")\n",
        "\n",
        "# Print learned fusion weights\n",
        "    if hasattr(model.fusion, 'get_weights'):\n",
        "       # weights = model.fusion.get_weights().cpu().numpy() # << OLD LINE\n",
        "       weights = model.fusion.get_weights().detach().cpu().numpy() # << CORRECTED LINE\n",
        "       weights_str = ', '.join([f'{w:.3f}' for w in weights])\n",
        "       print(f\"    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [{weights_str}]\")\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "# --- Evaluation Function (Metrics Calculation) ---\n",
        "def evaluate(model, dataloader, device, criterion_cls):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = [] # For AUC\n",
        "    total_val_loss = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in dataloader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            batch_size = imgs.size(0)\n",
        "\n",
        "            # Use the main forward pass for evaluation\n",
        "            outputs = model(imgs)\n",
        "\n",
        "            # Calculate validation loss based on the final fused output\n",
        "            loss = criterion_cls(outputs['fused_logits'], labels)\n",
        "            # Accumulate loss correctly weighted by batch size\n",
        "            total_val_loss += loss.item() * batch_size\n",
        "            num_samples += batch_size\n",
        "\n",
        "            # Get predictions and probabilities from fused logits\n",
        "            probs = F.softmax(outputs['fused_logits'], dim=1)\n",
        "            preds = torch.argmax(probs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            # Store probability of the positive class (assuming class 1 is 'real')\n",
        "            if probs.shape[1] > 1:\n",
        "                 all_probs.extend(probs[:, 1].cpu().numpy())\n",
        "            else: # Handle single output case if it ever happens\n",
        "                 all_probs.extend(probs[:, 0].cpu().numpy())\n",
        "\n",
        "\n",
        "    # Compute metrics\n",
        "    avg_val_loss = total_val_loss / num_samples if num_samples > 0 else 0\n",
        "    all_labels = np.array(all_labels)\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_probs = np.array(all_probs)\n",
        "\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    # Calculate binary metrics (assuming class 1 is positive)\n",
        "    prec = precision_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
        "    rec = recall_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
        "    f1 = f1_score(all_labels, all_preds, pos_label=1, zero_division=0)\n",
        "\n",
        "    auc = 0.0\n",
        "    try:\n",
        "        # Ensure there are samples from both classes for AUC\n",
        "        if len(np.unique(all_labels)) > 1 and len(all_probs) == len(all_labels):\n",
        "             auc = roc_auc_score(all_labels, all_probs)\n",
        "        elif len(np.unique(all_labels)) <= 1:\n",
        "             # print(\"Warning: Only one class present in evaluation data, AUC is not defined.\")\n",
        "             auc = 0.0 # Or handle as NaN, but 0.0 avoids issues downstream\n",
        "        else:\n",
        "             print(\"Warning: Mismatch between labels and probabilities count for AUC calculation.\")\n",
        "             auc = 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Could not calculate AUC: {e}\")\n",
        "        auc = 0.0\n",
        "\n",
        "    return avg_val_loss, acc, prec, rec, f1, auc\n",
        "\n",
        "# --- END OF CELL BLOCK ---"
      ],
      "metadata": {
        "id": "2cdHbxSoeA_8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training\n"
      ],
      "metadata": {
        "id": "5EbRtCa-eGAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the HDE model\n",
        "latent_dim = 128\n",
        "model = HDE(latent_dim=latent_dim, image_size=IMAGE_SIZE).to(device)\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5) # Added weight decay\n",
        "criterion_cls = nn.CrossEntropyLoss()\n",
        "\n",
        "# Loss weights and contrastive temperature\n",
        "lambda_rec = 1.0\n",
        "lambda_adv = 0.1\n",
        "lambda_ctr = 0.5 # <<< Adjusted contrastive weight\n",
        "contrastive_temp = 0.1 # <<< Contrastive temperature\n",
        "\n",
        "num_epochs = 20\n",
        "\n",
        "# --- Training Loop ---\n",
        "print(\"Starting Training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, device, criterion_cls,\n",
        "                             lambda_rec=lambda_rec, lambda_adv=lambda_adv, lambda_ctr=lambda_ctr,\n",
        "                             contrastive_temp=contrastive_temp)\n",
        "\n",
        "    val_loss, val_acc, val_prec, val_rec, val_f1, val_auc = evaluate(model, val_loader, device, criterion_cls)\n",
        "\n",
        "    # Keep printing train loss breakdown in train_epoch function\n",
        "    # print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"  Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}, Prec: {val_prec:.4f}, Rec: {val_rec:.4f}, F1: {val_f1:.4f}, AUC: {val_auc:.4f}\")\n",
        "\n",
        "print(\"Training Finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3161QuAeIXr",
        "outputId": "19e86863-adf3-4348-c4c5-5fdca2fd0e3e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training...\n",
            "Epoch 1/20:\n",
            "    Batch 100/1563, Current Avg Loss: 3.1945\n",
            "    Batch 200/1563, Current Avg Loss: 2.9125\n",
            "    Batch 300/1563, Current Avg Loss: 2.7260\n",
            "    Batch 400/1563, Current Avg Loss: 2.5916\n",
            "    Batch 500/1563, Current Avg Loss: 2.5026\n",
            "    Batch 600/1563, Current Avg Loss: 2.4297\n",
            "    Batch 700/1563, Current Avg Loss: 2.3719\n",
            "    Batch 800/1563, Current Avg Loss: 2.3275\n",
            "    Batch 900/1563, Current Avg Loss: 2.2920\n",
            "    Batch 1000/1563, Current Avg Loss: 2.2595\n",
            "    Batch 1100/1563, Current Avg Loss: 2.2309\n",
            "    Batch 1200/1563, Current Avg Loss: 2.2011\n",
            "    Batch 1300/1563, Current Avg Loss: 2.1769\n",
            "    Batch 1400/1563, Current Avg Loss: 2.1569\n",
            "    Batch 1500/1563, Current Avg Loss: 2.1384\n",
            "    Avg Train Loss: 2.1276 (Duration: 2996.76s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.4872, Fusion:0.4922] REC[Drgan:0.3464, Nash:0.1364] CTR:1.1923 ADV:0.6932\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.212, 0.192, 0.212, 0.191, 0.193]\n",
            "  Val Loss: 0.3888, Acc: 0.8246, Prec: 0.8011, Rec: 0.8635, F1: 0.8311, AUC: 0.9098\n",
            "Epoch 2/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.8691\n",
            "    Batch 200/1563, Current Avg Loss: 1.8550\n",
            "    Batch 300/1563, Current Avg Loss: 1.8485\n",
            "    Batch 400/1563, Current Avg Loss: 1.8428\n",
            "    Batch 500/1563, Current Avg Loss: 1.8356\n",
            "    Batch 600/1563, Current Avg Loss: 1.8290\n",
            "    Batch 700/1563, Current Avg Loss: 1.8255\n",
            "    Batch 800/1563, Current Avg Loss: 1.8212\n",
            "    Batch 900/1563, Current Avg Loss: 1.8218\n",
            "    Batch 1000/1563, Current Avg Loss: 1.8161\n",
            "    Batch 1100/1563, Current Avg Loss: 1.8152\n",
            "    Batch 1200/1563, Current Avg Loss: 1.8139\n",
            "    Batch 1300/1563, Current Avg Loss: 1.8135\n",
            "    Batch 1400/1563, Current Avg Loss: 1.8077\n",
            "    Batch 1500/1563, Current Avg Loss: 1.8041\n",
            "    Avg Train Loss: 1.8033 (Duration: 2975.21s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.4058, Fusion:0.4069] REC[Drgan:0.3208, Nash:0.1015] CTR:0.9978 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.214, 0.191, 0.214, 0.189, 0.192]\n",
            "  Val Loss: 0.3452, Acc: 0.8504, Prec: 0.8624, Rec: 0.8337, F1: 0.8478, AUC: 0.9270\n",
            "Epoch 3/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.6998\n",
            "    Batch 200/1563, Current Avg Loss: 1.7122\n",
            "    Batch 300/1563, Current Avg Loss: 1.7275\n",
            "    Batch 400/1563, Current Avg Loss: 1.7139\n",
            "    Batch 500/1563, Current Avg Loss: 1.7143\n",
            "    Batch 600/1563, Current Avg Loss: 1.7228\n",
            "    Batch 700/1563, Current Avg Loss: 1.7286\n",
            "    Batch 800/1563, Current Avg Loss: 1.7299\n",
            "    Batch 900/1563, Current Avg Loss: 1.7288\n",
            "    Batch 1000/1563, Current Avg Loss: 1.7251\n",
            "    Batch 1100/1563, Current Avg Loss: 1.7214\n",
            "    Batch 1200/1563, Current Avg Loss: 1.7226\n",
            "    Batch 1300/1563, Current Avg Loss: 1.7182\n",
            "    Batch 1400/1563, Current Avg Loss: 1.7197\n",
            "    Batch 1500/1563, Current Avg Loss: 1.7181\n",
            "    Avg Train Loss: 1.7166 (Duration: 2984.96s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3819, Fusion:0.3824] REC[Drgan:0.3142, Nash:0.0859] CTR:0.9661 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.215, 0.191, 0.215, 0.188, 0.192]\n",
            "  Val Loss: 0.3261, Acc: 0.8605, Prec: 0.9104, Rec: 0.7996, F1: 0.8514, AUC: 0.9416\n",
            "Epoch 4/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.6516\n",
            "    Batch 200/1563, Current Avg Loss: 1.6682\n",
            "    Batch 300/1563, Current Avg Loss: 1.6861\n",
            "    Batch 400/1563, Current Avg Loss: 1.6834\n",
            "    Batch 500/1563, Current Avg Loss: 1.6784\n",
            "    Batch 600/1563, Current Avg Loss: 1.6782\n",
            "    Batch 700/1563, Current Avg Loss: 1.6754\n",
            "    Batch 800/1563, Current Avg Loss: 1.6825\n",
            "    Batch 900/1563, Current Avg Loss: 1.6773\n",
            "    Batch 1000/1563, Current Avg Loss: 1.6764\n",
            "    Batch 1100/1563, Current Avg Loss: 1.6724\n",
            "    Batch 1200/1563, Current Avg Loss: 1.6714\n",
            "    Batch 1300/1563, Current Avg Loss: 1.6673\n",
            "    Batch 1400/1563, Current Avg Loss: 1.6674\n",
            "    Batch 1500/1563, Current Avg Loss: 1.6669\n",
            "    Avg Train Loss: 1.6669 (Duration: 2974.95s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3687, Fusion:0.3685] REC[Drgan:0.3091, Nash:0.0756] CTR:0.9513 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.217, 0.191, 0.216, 0.186, 0.191]\n",
            "  Val Loss: 0.3042, Acc: 0.8687, Prec: 0.8865, Rec: 0.8456, F1: 0.8656, AUC: 0.9442\n",
            "Epoch 5/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.6539\n",
            "    Batch 200/1563, Current Avg Loss: 1.6346\n",
            "    Batch 300/1563, Current Avg Loss: 1.6295\n",
            "    Batch 400/1563, Current Avg Loss: 1.6362\n",
            "    Batch 500/1563, Current Avg Loss: 1.6442\n",
            "    Batch 600/1563, Current Avg Loss: 1.6436\n",
            "    Batch 700/1563, Current Avg Loss: 1.6482\n",
            "    Batch 800/1563, Current Avg Loss: 1.6471\n",
            "    Batch 900/1563, Current Avg Loss: 1.6468\n",
            "    Batch 1000/1563, Current Avg Loss: 1.6433\n",
            "    Batch 1100/1563, Current Avg Loss: 1.6460\n",
            "    Batch 1200/1563, Current Avg Loss: 1.6434\n",
            "    Batch 1300/1563, Current Avg Loss: 1.6416\n",
            "    Batch 1400/1563, Current Avg Loss: 1.6398\n",
            "    Batch 1500/1563, Current Avg Loss: 1.6408\n",
            "    Avg Train Loss: 1.6461 (Duration: 2968.14s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3638, Fusion:0.3630] REC[Drgan:0.3051, Nash:0.0717] CTR:0.9462 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.217, 0.191, 0.216, 0.186, 0.191]\n",
            "  Val Loss: 0.4256, Acc: 0.8075, Prec: 0.9553, Rec: 0.6452, F1: 0.7702, AUC: 0.9429\n",
            "Epoch 6/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.6092\n",
            "    Batch 200/1563, Current Avg Loss: 1.5991\n",
            "    Batch 300/1563, Current Avg Loss: 1.6003\n",
            "    Batch 400/1563, Current Avg Loss: 1.6051\n",
            "    Batch 500/1563, Current Avg Loss: 1.5982\n",
            "    Batch 600/1563, Current Avg Loss: 1.5946\n",
            "    Batch 700/1563, Current Avg Loss: 1.5966\n",
            "    Batch 800/1563, Current Avg Loss: 1.6010\n",
            "    Batch 900/1563, Current Avg Loss: 1.5979\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5984\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5990\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5990\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5967\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5950\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5967\n",
            "    Avg Train Loss: 1.5958 (Duration: 2966.76s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3474, Fusion:0.3456] REC[Drgan:0.3009, Nash:0.0657] CTR:0.9338 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.218, 0.190, 0.216, 0.185, 0.190]\n",
            "  Val Loss: 0.3551, Acc: 0.8440, Prec: 0.9390, Rec: 0.7359, F1: 0.8251, AUC: 0.9463\n",
            "Epoch 7/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.5953\n",
            "    Batch 200/1563, Current Avg Loss: 1.6051\n",
            "    Batch 300/1563, Current Avg Loss: 1.5931\n",
            "    Batch 400/1563, Current Avg Loss: 1.5992\n",
            "    Batch 500/1563, Current Avg Loss: 1.5926\n",
            "    Batch 600/1563, Current Avg Loss: 1.5933\n",
            "    Batch 700/1563, Current Avg Loss: 1.5982\n",
            "    Batch 800/1563, Current Avg Loss: 1.5993\n",
            "    Batch 900/1563, Current Avg Loss: 1.5964\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5950\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5893\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5893\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5847\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5851\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5847\n",
            "    Avg Train Loss: 1.5832 (Duration: 2980.55s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3453, Fusion:0.3435] REC[Drgan:0.2975, Nash:0.0628] CTR:0.9296 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.219, 0.190, 0.216, 0.185, 0.190]\n",
            "  Val Loss: 0.3474, Acc: 0.8496, Prec: 0.9450, Rec: 0.7423, F1: 0.8315, AUC: 0.9494\n",
            "Epoch 8/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.5820\n",
            "    Batch 200/1563, Current Avg Loss: 1.5780\n",
            "    Batch 300/1563, Current Avg Loss: 1.5611\n",
            "    Batch 400/1563, Current Avg Loss: 1.5544\n",
            "    Batch 500/1563, Current Avg Loss: 1.5569\n",
            "    Batch 600/1563, Current Avg Loss: 1.5540\n",
            "    Batch 700/1563, Current Avg Loss: 1.5613\n",
            "    Batch 800/1563, Current Avg Loss: 1.5615\n",
            "    Batch 900/1563, Current Avg Loss: 1.5536\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5553\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5609\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5602\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5583\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5571\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5535\n",
            "    Avg Train Loss: 1.5549 (Duration: 2963.98s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3357, Fusion:0.3341] REC[Drgan:0.2942, Nash:0.0605] CTR:0.9221 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.220, 0.190, 0.217, 0.184, 0.189]\n",
            "  Val Loss: 0.3919, Acc: 0.8282, Prec: 0.9634, Rec: 0.6823, F1: 0.7989, AUC: 0.9519\n",
            "Epoch 9/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.5354\n",
            "    Batch 200/1563, Current Avg Loss: 1.5458\n",
            "    Batch 300/1563, Current Avg Loss: 1.5434\n",
            "    Batch 400/1563, Current Avg Loss: 1.5474\n",
            "    Batch 500/1563, Current Avg Loss: 1.5530\n",
            "    Batch 600/1563, Current Avg Loss: 1.5679\n",
            "    Batch 700/1563, Current Avg Loss: 1.5593\n",
            "    Batch 800/1563, Current Avg Loss: 1.5573\n",
            "    Batch 900/1563, Current Avg Loss: 1.5562\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5545\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5535\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5571\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5530\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5525\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5494\n",
            "    Avg Train Loss: 1.5472 (Duration: 2968.31s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3344, Fusion:0.3329] REC[Drgan:0.2914, Nash:0.0601] CTR:0.9181 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.220, 0.190, 0.217, 0.184, 0.189]\n",
            "  Val Loss: 0.2778, Acc: 0.8809, Prec: 0.8665, Rec: 0.9006, F1: 0.8832, AUC: 0.9542\n",
            "Epoch 10/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.5587\n",
            "    Batch 200/1563, Current Avg Loss: 1.5510\n",
            "    Batch 300/1563, Current Avg Loss: 1.5435\n",
            "    Batch 400/1563, Current Avg Loss: 1.5519\n",
            "    Batch 500/1563, Current Avg Loss: 1.5529\n",
            "    Batch 600/1563, Current Avg Loss: 1.5547\n",
            "    Batch 700/1563, Current Avg Loss: 1.5524\n",
            "    Batch 800/1563, Current Avg Loss: 1.5500\n",
            "    Batch 900/1563, Current Avg Loss: 1.5478\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5429\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5387\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5364\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5372\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5346\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5356\n",
            "    Avg Train Loss: 1.5345 (Duration: 2948.86s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3309, Fusion:0.3299] REC[Drgan:0.2897, Nash:0.0594] CTR:0.9106 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.221, 0.190, 0.218, 0.183, 0.189]\n",
            "  Val Loss: 0.2945, Acc: 0.8736, Prec: 0.8415, Rec: 0.9204, F1: 0.8792, AUC: 0.9528\n",
            "Epoch 11/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.5260\n",
            "    Batch 200/1563, Current Avg Loss: 1.5607\n",
            "    Batch 300/1563, Current Avg Loss: 1.5419\n",
            "    Batch 400/1563, Current Avg Loss: 1.5348\n",
            "    Batch 500/1563, Current Avg Loss: 1.5318\n",
            "    Batch 600/1563, Current Avg Loss: 1.5386\n",
            "    Batch 700/1563, Current Avg Loss: 1.5424\n",
            "    Batch 800/1563, Current Avg Loss: 1.5388\n",
            "    Batch 900/1563, Current Avg Loss: 1.5391\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5361\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5332\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5321\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5321\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5358\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5340\n",
            "    Avg Train Loss: 1.5331 (Duration: 2986.30s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3317, Fusion:0.3305] REC[Drgan:0.2889, Nash:0.0582] CTR:0.9091 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.221, 0.189, 0.219, 0.183, 0.188]\n",
            "  Val Loss: 0.3152, Acc: 0.8620, Prec: 0.8123, Rec: 0.9416, F1: 0.8722, AUC: 0.9541\n",
            "Epoch 12/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4954\n",
            "    Batch 200/1563, Current Avg Loss: 1.5466\n",
            "    Batch 300/1563, Current Avg Loss: 1.5458\n",
            "    Batch 400/1563, Current Avg Loss: 1.5423\n",
            "    Batch 500/1563, Current Avg Loss: 1.5278\n",
            "    Batch 600/1563, Current Avg Loss: 1.5189\n",
            "    Batch 700/1563, Current Avg Loss: 1.5208\n",
            "    Batch 800/1563, Current Avg Loss: 1.5236\n",
            "    Batch 900/1563, Current Avg Loss: 1.5211\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5184\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5187\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5188\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5175\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5149\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5151\n",
            "    Avg Train Loss: 1.5177 (Duration: 2978.32s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3264, Fusion:0.3252] REC[Drgan:0.2865, Nash:0.0569] CTR:0.9066 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.222, 0.189, 0.219, 0.182, 0.188]\n",
            "  Val Loss: 0.3341, Acc: 0.8548, Prec: 0.9566, Rec: 0.7433, F1: 0.8366, AUC: 0.9566\n",
            "Epoch 13/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4867\n",
            "    Batch 200/1563, Current Avg Loss: 1.4941\n",
            "    Batch 300/1563, Current Avg Loss: 1.5064\n",
            "    Batch 400/1563, Current Avg Loss: 1.5032\n",
            "    Batch 500/1563, Current Avg Loss: 1.5060\n",
            "    Batch 600/1563, Current Avg Loss: 1.5155\n",
            "    Batch 700/1563, Current Avg Loss: 1.5150\n",
            "    Batch 800/1563, Current Avg Loss: 1.5156\n",
            "    Batch 900/1563, Current Avg Loss: 1.5154\n",
            "    Batch 1000/1563, Current Avg Loss: 1.5156\n",
            "    Batch 1100/1563, Current Avg Loss: 1.5156\n",
            "    Batch 1200/1563, Current Avg Loss: 1.5167\n",
            "    Batch 1300/1563, Current Avg Loss: 1.5184\n",
            "    Batch 1400/1563, Current Avg Loss: 1.5150\n",
            "    Batch 1500/1563, Current Avg Loss: 1.5134\n",
            "    Avg Train Loss: 1.5160 (Duration: 2967.49s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3254, Fusion:0.3234] REC[Drgan:0.2849, Nash:0.0592] CTR:0.9075 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.222, 0.188, 0.220, 0.181, 0.188]\n",
            "  Val Loss: 0.3078, Acc: 0.8686, Prec: 0.9417, Rec: 0.7857, F1: 0.8567, AUC: 0.9556\n",
            "Epoch 14/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4819\n",
            "    Batch 200/1563, Current Avg Loss: 1.5033\n",
            "    Batch 300/1563, Current Avg Loss: 1.5028\n",
            "    Batch 400/1563, Current Avg Loss: 1.5008\n",
            "    Batch 500/1563, Current Avg Loss: 1.5033\n",
            "    Batch 600/1563, Current Avg Loss: 1.4937\n",
            "    Batch 700/1563, Current Avg Loss: 1.4958\n",
            "    Batch 800/1563, Current Avg Loss: 1.4940\n",
            "    Batch 900/1563, Current Avg Loss: 1.4943\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4921\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4930\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4893\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4907\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4878\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4893\n",
            "    Avg Train Loss: 1.4876 (Duration: 2982.70s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3157, Fusion:0.3126] REC[Drgan:0.2831, Nash:0.0568] CTR:0.9002 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.223, 0.188, 0.221, 0.181, 0.187]\n",
            "  Val Loss: 0.2767, Acc: 0.8821, Prec: 0.9046, Rec: 0.8544, F1: 0.8788, AUC: 0.9548\n",
            "Epoch 15/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4545\n",
            "    Batch 200/1563, Current Avg Loss: 1.4543\n",
            "    Batch 300/1563, Current Avg Loss: 1.4639\n",
            "    Batch 400/1563, Current Avg Loss: 1.4803\n",
            "    Batch 500/1563, Current Avg Loss: 1.4756\n",
            "    Batch 600/1563, Current Avg Loss: 1.4796\n",
            "    Batch 700/1563, Current Avg Loss: 1.4805\n",
            "    Batch 800/1563, Current Avg Loss: 1.4741\n",
            "    Batch 900/1563, Current Avg Loss: 1.4774\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4782\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4738\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4735\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4734\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4753\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4743\n",
            "    Avg Train Loss: 1.4737 (Duration: 2987.58s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3098, Fusion:0.3062] REC[Drgan:0.2821, Nash:0.0561] CTR:0.9002 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.224, 0.187, 0.222, 0.180, 0.187]\n",
            "  Val Loss: 0.2743, Acc: 0.8812, Prec: 0.8514, Rec: 0.9238, F1: 0.8861, AUC: 0.9591\n",
            "Epoch 16/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4543\n",
            "    Batch 200/1563, Current Avg Loss: 1.4505\n",
            "    Batch 300/1563, Current Avg Loss: 1.4538\n",
            "    Batch 400/1563, Current Avg Loss: 1.4549\n",
            "    Batch 500/1563, Current Avg Loss: 1.4505\n",
            "    Batch 600/1563, Current Avg Loss: 1.4667\n",
            "    Batch 700/1563, Current Avg Loss: 1.4628\n",
            "    Batch 800/1563, Current Avg Loss: 1.4632\n",
            "    Batch 900/1563, Current Avg Loss: 1.4634\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4653\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4685\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4683\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4663\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4666\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4654\n",
            "    Avg Train Loss: 1.4634 (Duration: 2986.97s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3067, Fusion:0.3018] REC[Drgan:0.2811, Nash:0.0568] CTR:0.8955 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.226, 0.186, 0.223, 0.179, 0.186]\n",
            "  Val Loss: 0.2687, Acc: 0.8864, Prec: 0.8723, Rec: 0.9055, F1: 0.8886, AUC: 0.9576\n",
            "Epoch 17/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4896\n",
            "    Batch 200/1563, Current Avg Loss: 1.4713\n",
            "    Batch 300/1563, Current Avg Loss: 1.4746\n",
            "    Batch 400/1563, Current Avg Loss: 1.4694\n",
            "    Batch 500/1563, Current Avg Loss: 1.4636\n",
            "    Batch 600/1563, Current Avg Loss: 1.4695\n",
            "    Batch 700/1563, Current Avg Loss: 1.4688\n",
            "    Batch 800/1563, Current Avg Loss: 1.4643\n",
            "    Batch 900/1563, Current Avg Loss: 1.4617\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4623\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4627\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4645\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4656\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4644\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4643\n",
            "    Avg Train Loss: 1.4637 (Duration: 2965.11s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.3081, Fusion:0.3026] REC[Drgan:0.2804, Nash:0.0561] CTR:0.8940 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.227, 0.185, 0.224, 0.178, 0.186]\n",
            "  Val Loss: 0.2619, Acc: 0.8894, Prec: 0.9202, Rec: 0.8528, F1: 0.8852, AUC: 0.9604\n",
            "Epoch 18/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4355\n",
            "    Batch 200/1563, Current Avg Loss: 1.4570\n",
            "    Batch 300/1563, Current Avg Loss: 1.4603\n",
            "    Batch 400/1563, Current Avg Loss: 1.4509\n",
            "    Batch 500/1563, Current Avg Loss: 1.4513\n",
            "    Batch 600/1563, Current Avg Loss: 1.4505\n",
            "    Batch 700/1563, Current Avg Loss: 1.4484\n",
            "    Batch 800/1563, Current Avg Loss: 1.4465\n",
            "    Batch 900/1563, Current Avg Loss: 1.4459\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4439\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4445\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4459\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4447\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4448\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4450\n",
            "    Avg Train Loss: 1.4462 (Duration: 2982.02s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.2996, Fusion:0.2949] REC[Drgan:0.2793, Nash:0.0563] CTR:0.8935 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.228, 0.185, 0.224, 0.178, 0.186]\n",
            "  Val Loss: 0.2760, Acc: 0.8832, Prec: 0.9337, Rec: 0.8250, F1: 0.8760, AUC: 0.9608\n",
            "Epoch 19/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4565\n",
            "    Batch 200/1563, Current Avg Loss: 1.4573\n",
            "    Batch 300/1563, Current Avg Loss: 1.4480\n",
            "    Batch 400/1563, Current Avg Loss: 1.4512\n",
            "    Batch 500/1563, Current Avg Loss: 1.4603\n",
            "    Batch 600/1563, Current Avg Loss: 1.4529\n",
            "    Batch 700/1563, Current Avg Loss: 1.4482\n",
            "    Batch 800/1563, Current Avg Loss: 1.4438\n",
            "    Batch 900/1563, Current Avg Loss: 1.4429\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4466\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4473\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4497\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4482\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4463\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4458\n",
            "    Avg Train Loss: 1.4446 (Duration: 2997.46s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.2987, Fusion:0.2933] REC[Drgan:0.2788, Nash:0.0583] CTR:0.8926 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.229, 0.184, 0.225, 0.177, 0.185]\n",
            "  Val Loss: 0.2506, Acc: 0.8945, Prec: 0.9174, Rec: 0.8671, F1: 0.8915, AUC: 0.9629\n",
            "Epoch 20/20:\n",
            "    Batch 100/1563, Current Avg Loss: 1.4191\n",
            "    Batch 200/1563, Current Avg Loss: 1.4177\n",
            "    Batch 300/1563, Current Avg Loss: 1.4294\n",
            "    Batch 400/1563, Current Avg Loss: 1.4299\n",
            "    Batch 500/1563, Current Avg Loss: 1.4325\n",
            "    Batch 600/1563, Current Avg Loss: 1.4286\n",
            "    Batch 700/1563, Current Avg Loss: 1.4299\n",
            "    Batch 800/1563, Current Avg Loss: 1.4342\n",
            "    Batch 900/1563, Current Avg Loss: 1.4402\n",
            "    Batch 1000/1563, Current Avg Loss: 1.4402\n",
            "    Batch 1100/1563, Current Avg Loss: 1.4403\n",
            "    Batch 1200/1563, Current Avg Loss: 1.4386\n",
            "    Batch 1300/1563, Current Avg Loss: 1.4366\n",
            "    Batch 1400/1563, Current Avg Loss: 1.4345\n",
            "    Batch 1500/1563, Current Avg Loss: 1.4333\n",
            "    Avg Train Loss: 1.4333 (Duration: 2963.43s)\n",
            "      Breakdown (Avg): CLS[Drgan:0.2930, Fusion:0.2884] REC[Drgan:0.2773, Nash:0.0589] CTR:0.8929 ADV:0.6931\n",
            "    Fusion Weights (Sem, Freq, Bio, Ctr, Nash): [0.231, 0.183, 0.225, 0.176, 0.185]\n",
            "  Val Loss: 0.2423, Acc: 0.8979, Prec: 0.9066, Rec: 0.8872, F1: 0.8968, AUC: 0.9646\n",
            "Training Finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing and Saving the Model"
      ],
      "metadata": {
        "id": "xZtBA2DyjYDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc, test_prec, test_rec, test_f1, test_auc = evaluate(model, test_loader, device, criterion_cls)\n",
        "print(\"Test Metrics:\")\n",
        "print(f\"  Loss: {test_loss:.4f}\")\n",
        "print(f\"  Accuracy: {test_acc:.4f}\")\n",
        "print(f\"  Precision: {test_prec:.4f}\")\n",
        "print(f\"  Recall: {test_rec:.4f}\")\n",
        "print(f\"  F1-score: {test_f1:.4f}\")\n",
        "print(f\"  AUC: {test_auc:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'hde_model.pth')\n",
        "print(\"Model saved as hde_model.pth\")\n"
      ],
      "metadata": {
        "id": "1AIiWKXAjZMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7397492-111d-43a5-ba9e-5952712fe348"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Metrics:\n",
            "  Loss: 0.2431\n",
            "  Accuracy: 0.8965\n",
            "  Precision: 0.9050\n",
            "  Recall: 0.8860\n",
            "  F1-score: 0.8954\n",
            "  AUC: 0.9643\n",
            "Model saved as hde_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Reload Data (Correct Path) ---\n",
        "IMAGE_SIZE = 256\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)\n",
        "])\n",
        "\n",
        "# Correct path for the test dataset as defined earlier\n",
        "test_data = datasets.ImageFolder('/content/dataset/real_vs_fake/test', transform=val_test_transforms) # <<< CORRECTED PATH\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=False, num_workers=4, pin_memory=True) # Added pin_memory\n",
        "\n",
        "\n",
        "# --- Load Model ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "latent_dim = 128\n",
        "model = HDE(latent_dim=latent_dim, image_size=IMAGE_SIZE) # Instantiate with image_size\n",
        "\n",
        "# Load the saved state dictionary\n",
        "model_path = 'hde_model.pth' # Use a new name for the enhanced model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device)) # Ensure correct device mapping\n",
        "model.to(device)\n",
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "\n",
        "# --- Evaluation Function (Ensure it's defined correctly) ---\n",
        "# Re-define or ensure the 'evaluate' function from Cell 28 is accessible here\n",
        "# It should take model, dataloader, device, criterion_cls\n",
        "# It should return: loss, acc, prec, rec, f1, auc\n",
        "# Using a simplified version here just for metrics calculation:\n",
        "\n",
        "def evaluate_metrics(model, dataloader, device):\n",
        "       model.eval()\n",
        "       all_preds = []\n",
        "       all_labels = []\n",
        "       with torch.no_grad():\n",
        "           for imgs, labels in dataloader:\n",
        "               imgs = imgs.to(device)\n",
        "               labels = labels.to(device)\n",
        "               outputs = model(imgs) # Model forward pass\n",
        "               # Use the fused output for final prediction\n",
        "               preds = torch.argmax(outputs['fused_logits'], dim=1)\n",
        "               all_preds.extend(preds.cpu().numpy())\n",
        "               all_labels.extend(labels.cpu().numpy())\n",
        "       # Compute metrics\n",
        "       acc = accuracy_score(all_labels, all_preds)\n",
        "       prec = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       rec = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
        "       try:\n",
        "           # Calculate probabilities for AUC if possible (requires softmax on logits)\n",
        "           # For simplicity using argmax preds here, AUC might be less reliable\n",
        "           auc = roc_auc_score(all_labels, all_preds)\n",
        "       except Exception as e:\n",
        "           print(f\"Could not calculate AUC: {e}\")\n",
        "           auc = 0.0\n",
        "       return acc, prec, rec, f1, auc\n",
        "\n",
        "# --- Run Evaluation ---\n",
        "print(\"Evaluating on Test Set...\")\n",
        "test_acc, test_prec, test_rec, test_f1, test_auc = evaluate_metrics(model, test_loader, device)\n",
        "\n",
        "print(\"\\nTest Metrics (Enhanced Model):\")\n",
        "print(f\"  Accuracy:  {test_acc:.4f}\")\n",
        "print(f\"  Precision: {test_prec:.4f}\")\n",
        "print(f\"  Recall:    {test_rec:.4f}\")\n",
        "print(f\"  F1-score:  {test_f1:.4f}\")\n",
        "print(f\"  AUC:       {test_auc:.4f}\")\n",
        "\n",
        "# --- Save the enhanced model ---\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"\\nEnhanced model saved as {model_path}\")"
      ],
      "metadata": {
        "id": "D8fde4eQEvQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7613b5f-5268-4e4f-cb69-cfc065e9b227"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on Test Set...\n",
            "\n",
            "Test Metrics (Enhanced Model):\n",
            "  Accuracy:  0.7197\n",
            "  Precision: 0.7495\n",
            "  Recall:    0.7197\n",
            "  F1-score:  0.7111\n",
            "  AUC:       0.7197\n",
            "\n",
            "Enhanced model saved as hde_model.pth\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}